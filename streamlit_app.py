import streamlit as st
import pandas as pd
import os
import sys
import warnings
import glob
import logging
import datetime
import urllib.parse
import webbrowser
import unicodedata
import re
from io import BytesIO

# Set up logging
log_dir = "logs"
os.makedirs(log_dir, exist_ok=True)
log_file = os.path.join(log_dir, f"streamlit_app_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# Log startup information
logging.info("="*50)
logging.info("STREAMLIT APP STARTED")
logging.info(f"Log file: {log_file}")
logging.info(f"Working directory: {os.getcwd()}")
logging.info(f"Python version: {sys.version}")
logging.info("="*50)

# Filter out the warning about print area
warnings.filterwarnings('ignore', message='Print area cannot be set to Defined name')

# Set page configuration
st.set_page_config(
    page_title="Checklistæ ¸å¯¹ç³»ç»Ÿ",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for styling
st.markdown("""
<style>
    .main-header {
        font-size: 2.2rem;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 0.5rem;
        margin-top: 0;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #0D47A1;
        margin-top: 1rem;
        margin-bottom: 1rem;
    }
    .success-message {
        color: #4CAF50;
        font-weight: bold;
    }
    .error-message {
        color: #F44336;
        font-weight: bold;
    }
    .warning-message {
        color: #FF9800;
        font-weight: bold;
    }

    /* æ–‡ä»¶ä¸Šä¼ åŒºåŸŸæ ·å¼ä¼˜åŒ– */
    .upload-section {
        background-color: #E3F2FD;
        padding: 1.5rem;
        border-radius: 12px;
        border: 1px solid #BBDEFB;
        margin-bottom: 1rem;
        min-height: 100px;
        display: flex;
        flex-direction: column;
        justify-content: center;
        text-align: center;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }

    .upload-section h3 {
        margin-top: 0;
        margin-bottom: 0.5rem;
        color: #1565C0;
        font-size: 1.2rem;
        font-weight: 600;
    }

    /* è®¾ç½®åŒºåŸŸæ ·å¼ */
    .settings-section {
        background-color: #F3E5F5;
        padding: 1.5rem;
        border-radius: 12px;
        border: 1px solid #E1BEE7;
        margin-bottom: 1rem;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }

    /* å¤„ç†æŒ‰é’®åŒºåŸŸæ ·å¼ */
    .process-section {
        background-color: #E8F5E8;
        padding: 1.5rem;
        border-radius: 12px;
        border: 1px solid #C8E6C9;
        margin-bottom: 1rem;
        text-align: center;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }

    /* å¸®åŠ©åŒºåŸŸæ ·å¼ */
    .help-section {
        background-color: #FFF3E0;
        padding: 1.5rem;
        border-radius: 12px;
        border: 1px solid #FFCC02;
        margin-bottom: 1rem;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }

    /* ç´§å‡‘çš„åˆ—å¸ƒå±€ */
    .stColumns {
        gap: 1.5rem !important;
    }

    /* å‡å°‘æ ‡ç­¾é¡µé—´è· */
    .stTabs [data-baseweb="tab-list"] {
        gap: 0.5rem;
        margin-bottom: 1rem;
    }

    /* ä¼˜åŒ–æ–‡ä»¶ä¸Šä¼ å™¨æ ·å¼ */
    .stFileUploader {
        margin-bottom: 0.5rem;
    }

    /* ä¼˜åŒ–æˆåŠŸå’Œä¿¡æ¯æ¶ˆæ¯æ ·å¼ */
    .stSuccess, .stInfo {
        margin-top: 0.5rem;
        margin-bottom: 0.5rem;
    }

    /* ä¼˜åŒ–è¿›åº¦æ¡æ ·å¼ */
    .stProgress {
        margin-bottom: 0.5rem;
    }

    /* é˜²æ­¢æŒ‰é’®ç‚¹å‡»åé¡µé¢è·³è½¬çš„æ ·å¼ */
    .stDownloadButton > button {
        position: relative;
        z-index: 1;
    }

    .stButton > button {
        position: relative;
        z-index: 1;
    }

    /* ä¿æŒé¡µé¢ä½ç½®çš„æ ·å¼ */
    .main .block-container {
        scroll-behavior: smooth;
        padding-top: 1rem;
    }

    /* æ”¹å–„æŒ‰é’®çš„è§†è§‰åé¦ˆ */
    .stDownloadButton > button:hover,
    .stButton > button:hover {
        transform: translateY(-1px);
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        transition: all 0.2s ease;
    }

    /* é˜²æ­¢é¡µé¢é‡æ–°åŠ è½½æ—¶çš„é—ªçƒ */
    .stApp {
        transition: none;
    }

    /* ä¼˜åŒ–ä¾§è¾¹æ æ ·å¼ */
    .css-1d391kg {
        padding-top: 1rem;
    }

    /* ä¾§è¾¹æ æ–‡ä»¶ä¸Šä¼ å™¨æ ·å¼ */
    .css-1d391kg .stFileUploader {
        margin-bottom: 1rem;
    }

    /* ä¾§è¾¹æ æŒ‰é’®æ ·å¼ */
    .css-1d391kg .stButton > button {
        width: 100%;
        margin-top: 0.5rem;
    }

    /* ä¾§è¾¹æ æ ‡é¢˜æ ·å¼ */
    .css-1d391kg h2, .css-1d391kg h3 {
        margin-top: 1rem;
        margin-bottom: 0.5rem;
    }

    /* ä¾§è¾¹æ æ»‘å—æ ·å¼ */
    .css-1d391kg .stSlider {
        margin-bottom: 0.5rem;
    }

    /* å‡å°‘é¡µé¢é¡¶éƒ¨é—´è· */
    .block-container {
        padding-top: 1rem;
        padding-bottom: 1rem;
    }

    /* å‡å°‘å„ä¸ªç»„ä»¶ä¹‹é—´çš„é—´è· */
    .element-container {
        margin-bottom: 0.5rem;
    }

    /* ä¼˜åŒ–æ ‡ç­¾é¡µå®¹å™¨ */
    .stTabs > div > div > div > div {
        padding-top: 1rem;
    }
</style>

<script>
// é˜²æ­¢æŒ‰é’®ç‚¹å‡»åé¡µé¢è·³è½¬åˆ°é¡¶éƒ¨
document.addEventListener('DOMContentLoaded', function() {
    // ç›‘å¬æ‰€æœ‰æŒ‰é’®ç‚¹å‡»äº‹ä»¶
    document.addEventListener('click', function(e) {
        if (e.target.tagName === 'BUTTON' || e.target.closest('button')) {
            // è®°å½•å½“å‰æ»šåŠ¨ä½ç½®
            const scrollPosition = window.pageYOffset || document.documentElement.scrollTop;

            // å»¶è¿Ÿæ¢å¤æ»šåŠ¨ä½ç½®
            setTimeout(function() {
                window.scrollTo(0, scrollPosition);
            }, 100);
        }
    });
});
</script>
""", unsafe_allow_html=True)

# Create directories if they don't exist
os.makedirs('input', exist_ok=True)
os.makedirs('output', exist_ok=True)

def normalize_filename(filename):
    """
    æ ‡å‡†åŒ–æ–‡ä»¶åï¼Œè§£å†³Macç³»ç»Ÿä¸­æ–‡ç¼–ç é—®é¢˜
    """
    if not filename:
        return filename

    try:
        # 1. Unicodeæ ‡å‡†åŒ– (NFD -> NFC)
        normalized = unicodedata.normalize('NFC', filename)

        # 2. ç§»é™¤æˆ–æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€ç‚¹å·ã€ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦
        safe_chars = re.sub(r'[^\w\u4e00-\u9fff\.\-_()]', '_', normalized)

        # 3. é™åˆ¶æ–‡ä»¶åé•¿åº¦
        name, ext = os.path.splitext(safe_chars)
        if len(name) > 100:  # é™åˆ¶ä¸»æ–‡ä»¶åé•¿åº¦
            name = name[:100]

        result = name + ext
        logging.info(f"Normalized filename: '{filename}' -> '{result}'")
        return result

    except Exception as e:
        logging.error(f"Error normalizing filename '{filename}': {str(e)}")
        # å¦‚æœæ ‡å‡†åŒ–å¤±è´¥ï¼Œä½¿ç”¨å®‰å…¨çš„è‹±æ–‡æ–‡ä»¶å
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        ext = os.path.splitext(filename)[1] if filename else '.xlsx'
        fallback_name = f"uploaded_file_{timestamp}{ext}"
        logging.info(f"Using fallback filename: '{fallback_name}'")
        return fallback_name

def safe_save_uploaded_file(uploaded_file, target_path):
    """
    å®‰å…¨ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶ï¼Œå¤„ç†æ–‡ä»¶åç¼–ç é—®é¢˜
    """
    try:
        # æ ‡å‡†åŒ–æ–‡ä»¶å
        safe_filename = normalize_filename(uploaded_file.name)

        # æ„å»ºå®‰å…¨çš„ç›®æ ‡è·¯å¾„
        target_dir = os.path.dirname(target_path)
        safe_target_path = os.path.join(target_dir, safe_filename)

        # ä¿å­˜æ–‡ä»¶
        with open(safe_target_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        logging.info(f"File saved successfully: {safe_target_path}")
        return safe_target_path, safe_filename

    except Exception as e:
        logging.error(f"Error saving uploaded file: {str(e)}")
        raise e

def create_safe_file_uploader(label, file_type=["xlsx"], key=None, accept_multiple_files=False):
    """
    åˆ›å»ºä¸€ä¸ªå®‰å…¨çš„æ–‡ä»¶ä¸Šä¼ å™¨ï¼Œå¤„ç†ä¸­æ–‡æ–‡ä»¶åé—®é¢˜
    """
    # æ·»åŠ æ–‡ä»¶åå»ºè®®
    help_text = """
    ğŸ’¡ æ–‡ä»¶åå»ºè®®ï¼š
    â€¢ é¿å…ä½¿ç”¨ç‰¹æ®Šå­—ç¬¦å’Œç©ºæ ¼
    â€¢ æ¨èä½¿ç”¨è‹±æ–‡ã€æ•°å­—ã€ä¸‹åˆ’çº¿
    â€¢ å¦‚ï¼šinvoice_20250127.xlsx
    """

    # åˆ›å»ºæ–‡ä»¶ä¸Šä¼ å™¨
    uploaded_files = st.file_uploader(
        label,
        type=file_type,
        key=key,
        accept_multiple_files=accept_multiple_files,
        help=help_text
    )

    return uploaded_files

# Main header
st.markdown("<h1 class='main-header'>Checklistæ ¸å¯¹ç³»ç»Ÿ</h1>", unsafe_allow_html=True)

# Sidebar
with st.sidebar:
    st.image("https://img.icons8.com/color/96/000000/invoice.png", width=80)
    st.markdown("## Checklistæ ¸å¯¹ç³»ç»Ÿ")
    st.markdown("**ç‰ˆæœ¬ 1.2**")

    st.markdown("---")

    # æ˜¾ç¤ºå½“å‰çŠ¶æ€
    st.markdown("### å½“å‰çŠ¶æ€")

    # æ£€æŸ¥æ–‡ä»¶ä¸Šä¼ çŠ¶æ€ï¼ˆä½¿ç”¨session stateï¼‰
    duty_uploaded = 'duty_rate_uploaded' in st.session_state and st.session_state.duty_rate_uploaded
    checklist_uploaded = 'checklist_uploaded' in st.session_state and st.session_state.checklist_uploaded
    invoices_uploaded = 'invoices_uploaded' in st.session_state and st.session_state.invoices_uploaded

    # æ–‡ä»¶ä¸Šä¼ çŠ¶æ€æ˜¾ç¤º
    if duty_uploaded:
        st.success("âœ… ç¨ç‡æ–‡ä»¶å·²ä¸Šä¼ ")
    else:
        st.info("â³ ç­‰å¾…ç¨ç‡æ–‡ä»¶")

    if checklist_uploaded:
        st.success("âœ… æ ¸å¯¹æ¸…å•å·²ä¸Šä¼ ")
    else:
        st.info("â³ ç­‰å¾…æ ¸å¯¹æ¸…å•")

    if invoices_uploaded:
        st.success("âœ… å‘ç¥¨æ–‡ä»¶å·²ä¸Šä¼ ")
    else:
        st.info("â³ ç­‰å¾…å‘ç¥¨æ–‡ä»¶")

    # æ˜¾ç¤ºæ•´ä½“è¿›åº¦
    total_files = 3
    uploaded_files = sum([duty_uploaded, checklist_uploaded, invoices_uploaded])
    progress = uploaded_files / total_files

    st.markdown("### ğŸ“ˆ ä¸Šä¼ è¿›åº¦")
    st.progress(progress)
    st.caption(f"å·²ä¸Šä¼  {uploaded_files}/{total_files} ä¸ªæ–‡ä»¶")

    st.markdown("---")

    # å¿«é€Ÿå¯¼èˆª
    st.markdown("### ğŸ§­ å¿«é€Ÿå¯¼èˆª")
    st.markdown("""
    - ğŸ“ **æ–‡ä»¶ä¸Šä¼ ä¸è®¾ç½®** - ä¸Šä¼ æ–‡ä»¶å¹¶é…ç½®å‚æ•°
    - ğŸ‘€ **æ•°æ®é¢„è§ˆ** - æŸ¥çœ‹ä¸Šä¼ çš„æ•°æ®
    - ğŸ“Š **å¤„ç†ç»“æœ** - æŸ¥çœ‹å¤„ç†åçš„æ•°æ®
    - ğŸ“‹ **å·®å¼‚æŠ¥å‘Š** - æŸ¥çœ‹æ¯”å¯¹å·®å¼‚
    - ğŸ“ **æ—¥å¿—** - æŸ¥çœ‹å¤„ç†æ—¥å¿—
    """)

    st.markdown("---")
    st.markdown("### â„¹ï¸ ç³»ç»Ÿä¿¡æ¯")
    st.caption("Â© 2025 Checklistæ ¸å¯¹ç³»ç»Ÿ")
    st.caption("ä¸“ä¸šçš„å‘ç¥¨æ ¸å¯¹è§£å†³æ–¹æ¡ˆ")

# Main content area with tabs
tab1, tab2, tab3, tab4, tab5 = st.tabs(["æ–‡ä»¶ä¸Šä¼ ", "æ•°æ®é¢„è§ˆ", "å¤„ç†ç»“æœ", "å·®å¼‚æŠ¥å‘Š", "æ—¥å¿—"])

def normalize_item_name(item_name):
    """
    æ ‡å‡†åŒ–Item Nameï¼Œç”¨äºæ›´å¥½çš„åŒ¹é…
    """
    if pd.isna(item_name) or item_name == '':
        return ''

    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶è½¬ä¸ºå¤§å†™
    normalized = str(item_name).upper()

    # ç§»é™¤å¸¸è§çš„ç‰¹æ®Šå­—ç¬¦å’Œç©ºæ ¼
    normalized = normalized.replace(' ', '').replace('-', '').replace('_', '')
    normalized = normalized.replace(',', '.').replace(';', '.')

    # ç§»é™¤å¸¸è§çš„åç¼€
    suffixes_to_remove = ['PARTNO', 'PART', 'NO', 'NUM', 'NUMBER']
    for suffix in suffixes_to_remove:
        if normalized.endswith(suffix):
            normalized = normalized[:-len(suffix)]

    return normalized.strip()

def find_best_match(item_name, duty_rates_dict):
    """
    ä¸ºç»™å®šçš„item_nameåœ¨duty_rateså­—å…¸ä¸­æ‰¾åˆ°æœ€ä½³åŒ¹é…
    """
    if not item_name or pd.isna(item_name):
        return None

    normalized_item = normalize_item_name(item_name)

    # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
    if item_name in duty_rates_dict:
        return item_name

    # å°è¯•æ ‡å‡†åŒ–åçš„ç²¾ç¡®åŒ¹é…
    for duty_item in duty_rates_dict.keys():
        if normalize_item_name(duty_item) == normalized_item:
            return duty_item

    # å°è¯•éƒ¨åˆ†åŒ¹é…ï¼ˆåŒ…å«å…³ç³»ï¼‰- æ”¹è¿›ç‰ˆæœ¬
    best_match = None
    best_score = 0

    for duty_item in duty_rates_dict.keys():
        normalized_duty = normalize_item_name(duty_item)

        # è®¡ç®—åŒ¹é…åˆ†æ•°
        score = 0

        # å®Œå…¨åŒ…å«å…³ç³»
        if normalized_item in normalized_duty:
            score = len(normalized_item) / len(normalized_duty)
        elif normalized_duty in normalized_item:
            score = len(normalized_duty) / len(normalized_item)
        else:
            # è®¡ç®—å…¬å…±å­ä¸²é•¿åº¦
            common_length = 0
            min_len = min(len(normalized_item), len(normalized_duty))
            for i in range(min_len):
                if normalized_item[i] == normalized_duty[i]:
                    common_length += 1
                else:
                    break

            # å¦‚æœæœ‰è¶³å¤Ÿé•¿çš„å…¬å…±å‰ç¼€ï¼Œä¹Ÿè®¤ä¸ºæ˜¯åŒ¹é…
            if common_length >= 8:  # è‡³å°‘8ä¸ªå­—ç¬¦çš„å…¬å…±å‰ç¼€
                score = common_length / max(len(normalized_item), len(normalized_duty))

        # æ›´æ–°æœ€ä½³åŒ¹é…
        if score > best_score and score >= 0.7:  # è‡³å°‘70%çš„åŒ¹é…åº¦
            best_score = score
            best_match = duty_item

    return best_match

# Functions from the original scripts
def get_duty_rates(file_path):
    logging.info(f"Reading duty rates from: {file_path}")
    try:
        # Read duty_rate.xlsx
        df = pd.read_excel(file_path)
        logging.info(f"Duty rate file loaded. Shape: {df.shape}")
        logging.info(f"Duty rate columns: {df.columns.tolist()}")

        # Group by Item_Name and aggregate other columns
        logging.info("Grouping duty rates by 'Item Name'")
        grouped_df = df.groupby('Item Name').agg({
            'HSN1': lambda x: ' '.join(str(i) for i in x if pd.notna(i)),  # Concatenate HSN1 with space
            'HSN2': lambda x: ' '.join(str(i) for i in x if pd.notna(i)),  # Add HSN2
            'Final BCD': 'min',
            'Final SWS': 'min',
            'Final IGST': 'min'
        }).reset_index()

        logging.info(f"Grouped duty rates. Shape: {grouped_df.shape}")

        # Convert DataFrame to dictionary
        duty_dict = {}
        for _, row in grouped_df.iterrows():
            # Use HSN2 if HSN1 is empty
            hsn_value = row['HSN1'] if pd.notna(row['HSN1']) and row['HSN1'].strip() != '' else row['HSN2']
            duty_dict[row['Item Name']] = {
                'hsn': hsn_value,
                'bcd': row['Final BCD'],
                'sws': row['Final SWS'],
                'igst': row['Final IGST']
            }

        logging.info(f"Created duty dictionary with {len(duty_dict)} items")
        return duty_dict, df
    except Exception as e:
        error_msg = f"è¯»å–ç¨ç‡æ–‡ä»¶å¤±è´¥: {str(e)}"
        logging.error(error_msg)
        logging.exception("Exception details:")
        st.error(error_msg)
        return {}, None

def process_invoice_file(file_path, duty_rates):
    logging.info(f"Processing invoice file: {file_path}")
    try:
        # Get all sheet names
        excel_file = pd.ExcelFile(file_path)
        all_sheet_names = excel_file.sheet_names
        logging.info(f"All sheet names in invoice file: {all_sheet_names}")

        # Store original sheet names for accessing sheets
        original_sheet_names = excel_file.sheet_names[1:]  # Skip the first sheet
        logging.info(f"Processing sheets (skipping first): {original_sheet_names}")

        # Process sheet names for display and ID creation (remove CI- prefix and trim spaces)
        processed_sheet_names = [name.replace('CI-', '').strip() if name.startswith('CI-') else name.strip()
                      for name in original_sheet_names]
        logging.info(f"Processed sheet names: {processed_sheet_names}")

        # Initialize DataFrames for results
        all_invoices_df = pd.DataFrame()
        new_descriptions_df = pd.DataFrame()

        # Process each sheet
        for i, (original_sheet_name, processed_sheet_name) in enumerate(zip(original_sheet_names, processed_sheet_names)):
            logging.info(f"Processing sheet {i+1}/{len(original_sheet_names)}: {original_sheet_name}")
            # Read the sheet
            df = pd.read_excel(file_path, sheet_name=original_sheet_name)
            logging.info(f"Sheet data loaded. Shape: {df.shape}")
            if not df.empty:
                logging.info(f"First few columns: {df.columns[:5].tolist() if len(df.columns) > 5 else df.columns.tolist()}")

            # åŠ¨æ€æ£€æµ‹åˆ—ç»“æ„
            # é¦–å…ˆå°è¯•æ‰¾åˆ°è¡¨å¤´è¡Œ
            header_row_idx = None
            for i in range(min(15, len(df))):
                row = df.iloc[i]
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…¸å‹çš„è¡¨å¤´å…³é”®è¯
                row_str = ' '.join([str(cell).upper() for cell in row if pd.notna(cell)])
                if any(keyword in row_str for keyword in ['QTY', 'QUANTITY', 'PRICE', 'AMOUNT', 'DESC', 'DESCRIPTION']):
                    header_row_idx = i
                    logging.info(f"Found potential header row at index {i}: {row_str}")
                    break
            
            # æ ¹æ®å®é™…çš„å‘ç¥¨æ–‡ä»¶ç»“æ„å®šä¹‰åˆ—ç´¢å¼•
            # å¦‚æœæ‰¾åˆ°è¡¨å¤´ï¼Œå°è¯•åŠ¨æ€æ˜ å°„ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤æ˜ å°„
            if header_row_idx is not None:
                header_row = df.iloc[header_row_idx]
                column_indices = {}
                
                for idx, cell in enumerate(header_row):
                    if pd.notna(cell):
                        cell_str = str(cell).upper().strip()
                        if any(keyword in cell_str for keyword in ['ITEM', 'NO', 'SL']) and 'Item#' not in column_indices:
                            column_indices['Item#'] = idx
                        elif any(keyword in cell_str for keyword in ['MODEL', 'PART']) and 'Model_No' not in column_indices:
                            column_indices['Model_No'] = idx
                        elif ('P/N' in cell_str or 'PN' in cell_str) and 'P/N' not in column_indices:
                            column_indices['P/N'] = idx
                        elif any(keyword in cell_str for keyword in ['DESC', 'DESCRIPTION']) and 'Desc' not in column_indices:
                            column_indices['Desc'] = idx
                        elif any(keyword in cell_str for keyword in ['COUNTRY', 'ORIGIN']) and 'Country' not in column_indices:
                            column_indices['Country'] = idx
                        elif any(keyword in cell_str for keyword in ['QTY', 'QUANTITY']) and 'Qty' not in column_indices:
                            column_indices['Qty'] = idx
                        elif any(keyword in cell_str for keyword in ['PRICE', 'RATE']) and 'Price' not in column_indices:
                            column_indices['Price'] = idx
                        elif any(keyword in cell_str for keyword in ['AMOUNT', 'TOTAL']) and 'Amount' not in column_indices:
                            column_indices['Amount'] = idx
                
                logging.info(f"Dynamic column mapping: {column_indices}")
                
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å¿…è¦çš„åˆ—éƒ½è¢«æ£€æµ‹åˆ°
                required_columns = ['Item#', 'Model_No', 'P/N', 'Desc', 'Qty', 'Price']
                missing_columns = [col for col in required_columns if col not in column_indices]
                
                if missing_columns:
                    logging.warning(f"Dynamic column detection failed, missing columns: {missing_columns}")
                    logging.warning("Using default mapping")
                    column_indices = {
                        'Item#': 0,      # Item number (1, 2, 3...)
                        'Model_No': 1,   # Model number (IPC-K7CP-3H1WE)
                        'P/N': 2,        # Part number (1.2.03.01.0002)
                        'Desc': 3,       # Description
                        'Country': 4,    # Country
                        'Qty': 5,        # Quantity
                        'Price': 6,      # Unit price
                        'Amount': 7,     # Total amount
                    }
                else:
                    # è¡¥å……å¯èƒ½ç¼ºå¤±çš„éå¿…è¦åˆ—
                    if 'Country' not in column_indices:
                        column_indices['Country'] = 4  # é»˜è®¤å€¼
                    if 'Amount' not in column_indices:
                        column_indices['Amount'] = 7   # é»˜è®¤å€¼
            else:
                logging.warning("No header row found, using default mapping")
                # ä½¿ç”¨é»˜è®¤çš„åˆ—ç´¢å¼•æ˜ å°„
                column_indices = {
                    'Item#': 0,      # Item number (1, 2, 3...)
                    'Model_No': 1,   # Model number (IPC-K7CP-3H1WE)
                    'P/N': 2,        # Part number (1.2.03.01.0002)
                    'Desc': 3,       # Description
                    'Country': 4,    # Country
                    'Qty': 5,        # Quantity
                    'Price': 6,      # Unit price
                    'Amount': 7,     # Total amount
                }

            logging.info(f"Using column indices for sheet {original_sheet_name}: {column_indices}")
            
            # æŸ¥æ‰¾æ•°æ®å¼€å§‹çš„è¡Œï¼ˆåœ¨è¡¨å¤´è¡Œä¹‹åï¼ŒåŒ…å«æ•°å­—çš„è¡Œï¼‰
            data_start_row = None
            search_start = header_row_idx + 1 if header_row_idx is not None else 0
            
            for row_idx in range(search_start, min(search_start + 20, len(df))):
                # æ£€æŸ¥ç¬¬ä¸€åˆ—æ˜¯å¦ä¸ºæ•°å­—ï¼ˆItem#ï¼‰
                if row_idx < len(df):
                    first_col_val = df.iloc[row_idx, 0]
                    if pd.notna(first_col_val) and str(first_col_val).strip().isdigit():
                        data_start_row = row_idx
                        logging.info(f"Found data starting at row {row_idx} in sheet {original_sheet_name}")
                        break
            
            if data_start_row is None:
                logging.warning(f"No data rows found in sheet {original_sheet_name}")
                continue
            
            # æå–æ•°æ®è¡Œ
            data_df = df.iloc[data_start_row:].copy()
            
            # é‡ç½®ç´¢å¼•
            data_df = data_df.reset_index(drop=True)

            # åˆ›å»ºæ–°çš„DataFrame
            sheet_df = pd.DataFrame()
            
            # åªå¤„ç†æœ‰æ•°æ®çš„è¡Œ
            valid_rows = []
            for idx, row in data_df.iterrows():
                # æ£€æŸ¥ç¬¬ä¸€åˆ—æ˜¯å¦ä¸ºæœ‰æ•ˆçš„Item#
                item_num = row.iloc[0] if len(row) > 0 else None
                if pd.notna(item_num) and str(item_num).strip().isdigit():
                    valid_rows.append(idx)
            
            if not valid_rows:
                logging.warning(f"No valid data rows found in sheet {original_sheet_name}")
                continue
            
            logging.info(f"Found {len(valid_rows)} valid data rows in sheet {original_sheet_name}")
            
            # æœ€ç»ˆå®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ‰€æœ‰å¿…è¦çš„åˆ—ç´¢å¼•éƒ½å­˜åœ¨
            required_keys = ['Item#', 'Model_No', 'P/N', 'Desc', 'Country', 'Qty', 'Price']
            for key in required_keys:
                if key not in column_indices:
                    logging.error(f"Missing required column index: {key}")
                    logging.error(f"Available column indices: {column_indices}")
                    logging.error("Falling back to default mapping")
                    column_indices = {
                        'Item#': 0,      # Item number (1, 2, 3...)
                        'Model_No': 1,   # Model number (IPC-K7CP-3H1WE)
                        'P/N': 2,        # Part number (1.2.03.01.0002)
                        'Desc': 3,       # Description
                        'Country': 4,    # Country
                        'Qty': 5,        # Quantity
                        'Price': 6,      # Unit price
                        'Amount': 7,     # Total amount
                    }
                    break
            
            # å¤„ç†æ¯ä¸ªæœ‰æ•ˆè¡Œ
            for row_idx in valid_rows:
                row = data_df.iloc[row_idx]
                
                # å®‰å…¨è·å–åˆ—å€¼
                def safe_get_col(col_idx):
                    if col_idx < len(row):
                        val = row.iloc[col_idx]
                        return val if pd.notna(val) else ''
                    return ''
                
                # æå–å„åˆ—æ•°æ®
                item_num = safe_get_col(column_indices['Item#'])
                part_num = safe_get_col(column_indices['P/N'])
                model_no = safe_get_col(column_indices['Model_No'])
                desc = safe_get_col(column_indices['Desc'])
                country = safe_get_col(column_indices['Country'])
                qty = safe_get_col(column_indices['Qty'])
                price = safe_get_col(column_indices['Price'])
                
                # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ï¼Œç‰¹åˆ«å…³æ³¨Qtyå­—æ®µ
                if row_idx < 3:  # åªè®°å½•å‰3è¡Œçš„è°ƒè¯•ä¿¡æ¯
                    logging.info(f"Row {row_idx} debugging:")
                    logging.info(f"  - Row length: {len(row)}")
                    logging.info(f"  - Qty column index: {column_indices.get('Qty', 'NOT_FOUND')}")
                    if 'Qty' in column_indices and column_indices['Qty'] < len(row):
                        raw_qty = row.iloc[column_indices['Qty']]
                        logging.info(f"  - Raw Qty value: '{raw_qty}' (type: {type(raw_qty)})")
                        logging.info(f"  - Processed Qty: '{qty}'")
                    else:
                        logging.info(f"  - Qty column index out of range or not found")
                    
                    if 'Price' in column_indices and column_indices['Price'] < len(row):
                        raw_price = row.iloc[column_indices['Price']]
                        logging.info(f"  - Raw Price value: '{raw_price}' (type: {type(raw_price)})")
                        logging.info(f"  - Processed Price: '{price}'")
                    else:
                        logging.info(f"  - Price column index out of range or not found")
                
                # å¤„ç†æè¿°å­—æ®µï¼Œæå–Item_Name
                item_name = ''
                if desc:
                    # ä»æè¿°ä¸­æå–Item_Nameï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ª'-'ä¹‹å‰çš„éƒ¨åˆ†ï¼‰
                    desc_str = str(desc)
                    if '-' in desc_str:
                        item_name = desc_str.split('-')[0].strip()
                    else:
                        item_name = desc_str.strip()
                
                # æ¸…ç†æè¿°æ–‡æœ¬
                clean_desc = ''
                if desc:
                    clean_desc = ''.join(char.upper() for char in str(desc) if char.isalnum() or char in '.()').replace('Î¦', '').replace('Î©', '').replace('-', '').replace('Ï†', '')
                
                # åˆ›å»ºID
                item_id = f"{processed_sheet_name}_{str(item_num).strip()}"
                
                # æ·»åŠ åˆ°ç»“æœDataFrame
                row_data = {
                    'Item#': str(item_num).strip(),
                    'ID': item_id,
                    'P/N': str(part_num).strip(),
                    'Desc': clean_desc,
                    'Qty': str(qty).strip(),
                    'Price': str(price).strip(),
                    'Item_Name': item_name,
                    'HSN': '',
                    'BCD': '',
                    'SWS': '',
                    'IGST': ''
                }
                
                sheet_df = pd.concat([sheet_df, pd.DataFrame([row_data])], ignore_index=True)
            
            logging.info(f"Processed {len(sheet_df)} items from sheet {original_sheet_name}")

            # å¡«å……ç¨ç‡ä¿¡æ¯å¹¶æ”¶é›†æœªåŒ¹é…çš„é¡¹ç›®
            unique_desc = set()
            new_items_count = 0
            
            for idx, row in sheet_df.iterrows():
                itemName = row['Item_Name']
                if pd.notna(itemName) and itemName != '':
                    # ä½¿ç”¨æ”¹è¿›çš„åŒ¹é…ç®—æ³•
                    matched_duty_item = find_best_match(itemName, duty_rates)

                    if matched_duty_item:
                        rates = duty_rates[matched_duty_item]
                        sheet_df.at[idx, 'HSN'] = str(rates['hsn'])
                        sheet_df.at[idx, 'BCD'] = str(rates['bcd'])
                        sheet_df.at[idx, 'SWS'] = str(rates['sws'])
                        sheet_df.at[idx, 'IGST'] = str(rates['igst'])

                        # è®°å½•åŒ¹é…ä¿¡æ¯ç”¨äºè°ƒè¯•
                        if matched_duty_item != itemName:
                            logging.info(f"Fuzzy match found: '{itemName}' -> '{matched_duty_item}'")
                    else:
                        sheet_df.at[idx, 'HSN'] = 'new item'
                        sheet_df.at[idx, 'BCD'] = 'new item'
                        sheet_df.at[idx, 'SWS'] = 'new item'
                        sheet_df.at[idx, 'IGST'] = 'new item'
                        if itemName not in unique_desc:
                            unique_desc.add(itemName)
                            new_items_count += 1
                            logging.info(f"No match found for item: '{itemName}' (normalized: '{normalize_item_name(itemName)}')")
                            # Add unmatched items to new_descriptions_df
                            new_descriptions_df = pd.concat([new_descriptions_df, pd.DataFrame({
                                'å‘ç¥¨åŠé¡¹å·': [str(row['ID'])],
                                'Item Name': [itemName],
                                'Final BCD': [''],
                                'Final SWS': [''],
                                'Final IGST': [''],
                                'HSN1': [''],
                                # æ·»åŠ å…¼å®¹æ—§ç‰ˆæœ¬çš„åˆ—ï¼Œç¡®ä¿ä¸ºå­—ç¬¦ä¸²ç±»å‹
                                'Duty': [''],
                                'Welfare': ['']
                            })], ignore_index=True)

            logging.info(f"Found {new_items_count} new items in sheet {original_sheet_name}")

            # Add this sheet's data to the combined DataFrame
            all_invoices_df = pd.concat([all_invoices_df, sheet_df], ignore_index=True)
            logging.info(f"Added sheet data to combined DataFrame. Current size: {all_invoices_df.shape}")

        logging.info(f"Completed processing invoice file. Final DataFrame shape: {all_invoices_df.shape}")
        logging.info(f"New items found: {len(new_descriptions_df)}")

        # Check for duplicate IDs
        duplicate_ids = all_invoices_df['ID'].value_counts()[all_invoices_df['ID'].value_counts() > 1]
        if not duplicate_ids.empty:
            logging.warning(f"Found {len(duplicate_ids)} duplicate IDs in processed invoices:")
            logging.warning(duplicate_ids.head(10).to_dict())  # Log first 10 duplicates
            # Remove duplicates
            all_invoices_df = all_invoices_df.drop_duplicates(subset=['ID'], keep='first')
            logging.info(f"Removed duplicates. New DataFrame shape: {all_invoices_df.shape}")

        # åœ¨returnå‰æ·»åŠ ç±»å‹è½¬æ¢
        # ç¡®ä¿æ‰€æœ‰åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼Œè§£å†³Arrowåºåˆ—åŒ–é—®é¢˜
        logging.info(f"Converting all columns to string type for invoices DataFrame. Columns: {all_invoices_df.columns.tolist()}")
        for col in all_invoices_df.columns:
            try:
                all_invoices_df[col] = all_invoices_df[col].astype(str)
            except Exception as e:
                logging.error(f"Error converting column '{col}' to string: {str(e)}")
                # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œå°è¯•å¼ºåˆ¶è½¬æ¢
                all_invoices_df[col] = all_invoices_df[col].apply(lambda x: str(x) if x is not None else '')

        # é¢å¤–å¤„ç†ï¼šç¡®ä¿æ²¡æœ‰å­—ç¬¦ä¸²å½¢å¼çš„'nan'æˆ–'none'
        for col in all_invoices_df.columns:
            all_invoices_df[col] = all_invoices_df[col].apply(
                lambda x: '' if str(x).lower() in ['nan', 'none'] else str(x)
            )

        logging.info("Completed type conversion for invoices DataFrame")

        # ç¡®ä¿new_descriptions_dfä¸­çš„æ‰€æœ‰åˆ—éƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œç‰¹åˆ«æ˜¯Dutyåˆ—
        if not new_descriptions_df.empty:
            # è®°å½•åˆ—åï¼Œå¸®åŠ©è°ƒè¯•
            logging.info(f"new_descriptions_df columns: {new_descriptions_df.columns.tolist()}")

            # é™¤äº†'Item Name'å¤–ï¼Œå°†æ‰€æœ‰åˆ—è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            for col in new_descriptions_df.columns:
                if col != 'Item Name':  # ä¿ç•™Item Nameåˆ—çš„åŸå§‹ç±»å‹
                    new_descriptions_df[col] = new_descriptions_df[col].astype(str)

            # ç‰¹åˆ«å¤„ç†å¯èƒ½å­˜åœ¨çš„Dutyå’ŒWelfareåˆ—ï¼ˆæ—§åç§°ï¼‰
            for col in ['Duty', 'Welfare', 'Final BCD', 'Final SWS', 'Final IGST', 'HSN1']:
                if col in new_descriptions_df.columns:
                    new_descriptions_df[col] = new_descriptions_df[col].astype(str)

        return all_invoices_df, new_descriptions_df
    except Exception as e:
        error_msg = f"å¤„ç†å‘ç¥¨æ–‡ä»¶å¤±è´¥: {str(e)}"
        logging.error(error_msg)
        logging.exception("Exception details:")
        st.error(error_msg)
        return pd.DataFrame(), pd.DataFrame()

def process_checklist(file_path):
    logging.info(f"Processing checklist file: {file_path}")
    try:
        # è¯»å–Excelæ–‡ä»¶
        df = pd.read_excel(file_path, skiprows=3)
        logging.info(f"Checklist file loaded. Shape: {df.shape}")
        logging.info(f"Checklist columns: {df.columns.tolist()}")

        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å°è¯•æ‰¾åˆ°ç›¸ä¼¼çš„åˆ—å
        required_columns = ['P/N', 'Item#', 'Desc', 'Qty', 'Price', 'HSN', 'BCD', 'SWS', 'IGST']
        column_mapping = {}

        for req_col in required_columns:
            if req_col in df.columns:
                column_mapping[req_col] = req_col
            else:
                # ç‰¹æ®Šå¤„ç†IGSTåˆ—ï¼Œå¯èƒ½å¯¹åº”Dutyåˆ—
                if req_col == 'IGST' and 'Duty' in df.columns:
                    column_mapping[req_col] = 'Duty'
                    logging.info(f"Mapped column '{req_col}' to 'Duty'")
                    continue
                
                # å°è¯•æ‰¾åˆ°ç›¸ä¼¼çš„åˆ—åï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼Œå¿½ç•¥ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦ï¼‰
                found = False
                for col in df.columns:
                    col_clean = str(col).strip().replace(' ', '').replace('/', '').replace('-', '').upper()
                    req_col_clean = req_col.replace('/', '').replace('-', '').upper()
                    if col_clean == req_col_clean or req_col_clean in col_clean:
                        column_mapping[req_col] = col
                        logging.info(f"Mapped column '{req_col}' to '{col}'")
                        found = True
                        break

                if not found:
                    logging.warning(f"Column '{req_col}' not found in checklist file")
                    column_mapping[req_col] = None

        logging.info(f"Column mapping: {column_mapping}")

        # åˆå§‹åŒ–æ–°çš„DataFrameç”¨äºå­˜å‚¨ç»“æœ
        result_rows = []
        current_invoice = None
        invoice_count = 0
        item_count = 0

        # éå†æ¯ä¸€è¡Œ
        for _, row in df.iterrows():  # ä½¿ç”¨ _ è¡¨ç¤ºä¸ä½¿ç”¨çš„ç´¢å¼•å˜é‡
            # æ£€æŸ¥æ˜¯å¦æ˜¯å‘ç¥¨è¡Œ - ä½¿ç”¨å®‰å…¨çš„åˆ—è®¿é—®
            pn_col = column_mapping.get('P/N')
            if pn_col and pn_col in df.columns:
                pn = str(row[pn_col]) if pd.notna(row[pn_col]) else ''
            else:
                # å¦‚æœæ²¡æœ‰P/Nåˆ—ï¼Œå°è¯•åœ¨ç¬¬ä¸€åˆ—æˆ–å…¶ä»–åˆ—ä¸­æŸ¥æ‰¾Invoiceä¿¡æ¯
                pn = ''
                for col in df.columns:
                    if pd.notna(row[col]) and 'Invoice:' in str(row[col]):
                        pn = str(row[col])
                        break

            if 'Invoice:' in str(pn):
                # æå–å‘ç¥¨å·å¹¶å»é™¤æ‰€æœ‰ç©ºæ ¼
                invoice_no = pn.split('Invoice:')[1].split('dt.')[0].strip().replace(' ', '')
                current_invoice = invoice_no
                invoice_count += 1
                logging.info(f"Found invoice #{invoice_count}: {invoice_no}")
                # ä¿å­˜å‘ç¥¨è¡Œ
                result_rows.append({
                    'Item#': pn,
                    'ID': None,
                    'P/N': None,
                    'Desc': None,
                    'Qty': None,
                    'Price': None,
                    'Item_Name': None,
                    'HSN': None,
                    'BCD': None,
                    'SWS': None,
                    'IGST': None
                })
            else:
                # å¤„ç†å¸¸è§„è¡Œ - ä½¿ç”¨å®‰å…¨çš„åˆ—è®¿é—®
                item_col = column_mapping.get('Item#')
                if item_col and item_col in df.columns and pd.notna(row[item_col]) and current_invoice:
                    # Convert Item# to integer before concatenating to remove the decimal and spaces
                    try:
                        item_num = int(float(row[item_col])) if pd.notna(row[item_col]) else 0
                        item_id = f"{current_invoice}_{item_num}"
                    except (ValueError, TypeError):
                        # Remove spaces from item number to ensure clean IDs
                        clean_item = str(row[item_col]).strip().replace(' ', '')
                        item_id = f"{current_invoice}_{clean_item}"

                    # å®‰å…¨è·å–Descåˆ—
                    desc_col = column_mapping.get('Desc')
                    if desc_col and desc_col in df.columns and pd.notna(row[desc_col]):
                        item_name = str(row[desc_col]).split('-')[0]
                        # ä¿®æ”¹item_nameçš„å¤„ç†é€»è¾‘
                        desc_parts = str(row[desc_col]).split('-PART NO')
                        desc = desc_parts[0]
                        # åªä¿ç•™å­—æ¯æ•°å­—å’Œç‚¹å·ï¼Œå¹¶è½¬æ¢ä¸ºå¤§å†™
                        desc = desc.replace('+OR-', '')
                        desc = desc.replace('DEG', '')
                        desc = desc.replace('-', '')
                        desc = ''.join(char for char in desc if char.isalnum() or char in '.()').upper()
                    else:
                        item_name = ''
                        desc = ''

                    item_count += 1

                    # åˆ›å»ºä¸€ä¸ªå®‰å…¨çš„å­—å…¸ï¼Œä½¿ç”¨æ˜ å°„çš„åˆ—å
                    def safe_get_value(col_name):
                        mapped_col = column_mapping.get(col_name)
                        if mapped_col and mapped_col in df.columns:
                            return row[mapped_col] if pd.notna(row[mapped_col]) else ''
                        return ''

                    item_dict = {
                        'Item#': safe_get_value('Item#'),
                        'ID': item_id,
                        'P/N': safe_get_value('P/N'),
                        'Desc': desc,
                        'Qty': safe_get_value('Qty'),
                        'Price': safe_get_value('Price'),
                        'Item_Name': item_name,
                        'HSN': '',
                        'BCD': '',
                        'SWS': '',
                        'IGST': '',
                    }

                    # å¤„ç†HSNåˆ—ï¼ˆå¯èƒ½æ˜¯æ•°å­—ï¼‰
                    hsn_value = safe_get_value('HSN')
                    if hsn_value and str(hsn_value).strip().isdigit():
                        item_dict['HSN'] = int(float(hsn_value))
                    else:
                        item_dict['HSN'] = hsn_value

                    # å¤„ç†å…¶ä»–æ•°å€¼åˆ—
                    for col in ['BCD', 'SWS', 'IGST']:
                        item_dict[col] = safe_get_value(col)

                    # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹
                    for key in ['Item#', 'P/N', 'ID', 'Qty', 'Price', 'HSN', 'BCD', 'SWS', 'IGST']:
                        if key in item_dict and item_dict[key] is not None:
                            item_dict[key] = str(item_dict[key])

                    result_rows.append(item_dict)

        # åˆ›å»ºç»“æœDataFrame
        result_df = pd.DataFrame(result_rows)
        logging.info(f"Processed checklist with {invoice_count} invoices and {item_count} items")
        logging.info(f"Final checklist DataFrame shape: {result_df.shape}")

        # å¦‚æœæ²¡æœ‰å¤„ç†åˆ°ä»»ä½•æ•°æ®ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
        if result_df.empty:
            logging.warning("No data was processed from checklist file")
            logging.warning(f"Available columns in file: {df.columns.tolist()}")
            logging.warning(f"Column mapping used: {column_mapping}")
            logging.warning("Please check if the file format matches expected structure")

            # æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®ä»¥å¸®åŠ©è°ƒè¯•
            if not df.empty:
                logging.warning(f"First few rows of data:")
                for i, row in df.head(5).iterrows():
                    logging.warning(f"Row {i}: {row.to_dict()}")
        else:
            logging.info(f"Successfully processed checklist data")
            if not result_df.empty:
                logging.info(f"Sample processed data: {result_df.head(2).to_dict()}")

        # Check for duplicate IDs
        duplicate_ids = result_df['ID'].value_counts()[result_df['ID'].value_counts() > 1]
        if not duplicate_ids.empty:
            logging.warning(f"Found {len(duplicate_ids)} duplicate IDs in processed checklist:")
            logging.warning(duplicate_ids.head(10).to_dict())  # Log first 10 duplicates
            # Remove duplicates
            result_df = result_df.drop_duplicates(subset=['ID'], keep='first')
            logging.info(f"Removed duplicates. New DataFrame shape: {result_df.shape}")

        # åœ¨returnå‰æ·»åŠ ç±»å‹è½¬æ¢
        # ç¡®ä¿æ‰€æœ‰åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼Œè§£å†³Arrowåºåˆ—åŒ–é—®é¢˜
        for col in result_df.columns:
            result_df[col] = result_df[col].astype(str)

        # é¢å¤–å¤„ç†ï¼šç¡®ä¿æ²¡æœ‰å­—ç¬¦ä¸²å½¢å¼çš„'nan'æˆ–'none'
        for col in result_df.columns:
            result_df[col] = result_df[col].apply(
                lambda x: '' if str(x).lower() in ['nan', 'none'] else str(x)
            )

        return result_df
    except Exception as e:
        error_msg = f"å¤„ç†æ ¸å¯¹æ¸…å•å¤±è´¥: {str(e)}"
        logging.error(error_msg)
        logging.exception("Exception details:")

        # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ç»™ç”¨æˆ·
        if "KeyError" in str(e):
            missing_col = str(e).split("'")[1] if "'" in str(e) else "æœªçŸ¥åˆ—"
            detailed_msg = f"æ ¸å¯¹æ¸…å•æ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—: '{missing_col}'"
            st.error(f"âŒ {detailed_msg}")
            st.info("ğŸ’¡ è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œç¡®ä¿åŒ…å«ä»¥ä¸‹åˆ—ï¼šItem#, P/N, Desc, Qty, Price, HSN, BCD, SWS, IGST")
        else:
            st.error(f"âŒ {error_msg}")
            st.info("ğŸ’¡ è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦ä¸ºæ­£ç¡®çš„Excelæ–‡ä»¶(.xlsx)")

        return pd.DataFrame()

def compare_excels(df1, df2, price_tolerance_pct=1.1):
    logging.info("Starting comparison between processed invoices and checklist")
    logging.info(f"Using price tolerance: {price_tolerance_pct}%")
    logging.info(f"DataFrame 1 (invoices) shape: {df1.shape}")
    logging.info(f"DataFrame 2 (checklist) shape: {df2.shape}")

    # è®°å½•ä¸¤ä¸ªDataFrameçš„åˆ—åï¼Œå¸®åŠ©è°ƒè¯•
    logging.info(f"DataFrame 1 columns: {df1.columns.tolist()}")
    logging.info(f"DataFrame 2 columns: {df2.columns.tolist()}")

    try:
        # æ£€æŸ¥IDåˆ—æ˜¯å¦å­˜åœ¨
        if 'ID' not in df1.columns:
            error_msg = "'ID' column not found in DataFrame 1"
            logging.error(error_msg)
            st.error(error_msg)
            return pd.DataFrame()

        if 'ID' not in df2.columns:
            error_msg = "'ID' column not found in DataFrame 2"
            logging.error(error_msg)
            st.error(error_msg)
            return pd.DataFrame()

        # æ£€æŸ¥ä¸ºnullçš„IDs
        null_ids_df1 = df1['ID'].isna().sum()
        null_ids_df2 = df2['ID'].isna().sum()
        if null_ids_df1 > 0:
            logging.warning(f"Found {null_ids_df1} null IDs in DataFrame 1 (invoices)")
        if null_ids_df2 > 0:
            logging.warning(f"Found {null_ids_df2} null IDs in DataFrame 2 (checklist)")

        # Check for duplicate IDs before comparison
        duplicate_ids_df1 = df1['ID'].value_counts()[df1['ID'].value_counts() > 1]
        duplicate_ids_df2 = df2['ID'].value_counts()[df2['ID'].value_counts() > 1]

        if not duplicate_ids_df1.empty:
            logging.warning(f"Found {len(duplicate_ids_df1)} duplicate IDs in DataFrame 1 (invoices)")
            logging.warning(f"First 5 duplicates: {duplicate_ids_df1.head().to_dict()}")
            # Remove duplicates from df1, keeping the first occurrence
            df1 = df1.drop_duplicates(subset=['ID'], keep='first')
            logging.info(f"Removed duplicates from DataFrame 1. New shape: {df1.shape}")

        if not duplicate_ids_df2.empty:
            logging.warning(f"Found {len(duplicate_ids_df2)} duplicate IDs in DataFrame 2 (checklist)")
            logging.warning(f"First 5 duplicates: {duplicate_ids_df2.head().to_dict()}")
            # Remove duplicates from df2, keeping the first occurrence
            df2 = df2.drop_duplicates(subset=['ID'], keep='first')
            logging.info(f"Removed duplicates from DataFrame 2. New shape: {df2.shape}")

        # ç¡®ä¿ä¸¤ä¸ª DataFrame çš„åˆ—åé¡ºåºä¸€è‡´
        # é¦–å…ˆæ£€æŸ¥ä¸¤ä¸ª DataFrame çš„åˆ—æ˜¯å¦ä¸€è‡´
        common_columns = list(set(df1.columns) & set(df2.columns))
        logging.info(f"Common columns between DataFrames: {common_columns}")

        # åªä½¿ç”¨ä¸¤ä¸ª DataFrame ä¸­éƒ½å­˜åœ¨çš„åˆ—è¿›è¡Œæ¯”è¾ƒ
        df1 = df1[common_columns]
        df2 = df2[common_columns]
        logging.info(f"Aligned columns between DataFrames")

        # ç”¨äºå­˜å‚¨å·®å¼‚ä¿¡æ¯
        diff_data = []
        # Keep track of processed IDs to avoid duplicates
        processed_ids = set()
        match_count = 0
        diff_count = 0

        for _, row1 in df1.iterrows():
            id1 = row1['ID']
            # Skip if this ID has already been processed or is None
            if id1 is None or pd.isna(id1) or id1 in processed_ids:
                continue

            # Add this ID to the processed set
            processed_ids.add(id1)

            # æŸ¥æ‰¾ df2 ä¸­å¯¹åº” ID çš„è¡Œ
            matching_row = df2[df2['ID'] == id1]
            if not matching_row.empty:
                match_count += 1
                row2 = matching_row.iloc[0]
                diff_cols = []
                for col in df1.columns[1:]:  # è·³è¿‡IDåˆ—
                    # è·³è¿‡Item_Nameåˆ—å’ŒItem#åˆ—çš„æ¯”å¯¹
                    if col == 'Item_Name' or col == 'Item#':
                        continue

                    # å¯¹Priceåˆ—ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„è¯¯å·®èŒƒå›´
                    if col == 'Price':
                        if pd.notna(row1[col]) and pd.notna(row2[col]):
                            try:
                                # é¦–å…ˆå°è¯•å°†å€¼è½¬æ¢ä¸ºfloatç±»å‹
                                price1 = float(row1[col]) if isinstance(row1[col], str) else row1[col]
                                price2 = float(row2[col]) if isinstance(row2[col], str) else row2[col]

                                # è®¡ç®—å®¹å·®
                                tolerance = price1 * (price_tolerance_pct / 100)  # è½¬æ¢ä¸ºå°æ•°
                                if abs(price1 - price2) > tolerance:
                                    diff_cols.append(col)
                            except (ValueError, TypeError):
                                # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶è·³è¿‡æ¯”è¾ƒ
                                logging.warning(f"æ— æ³•æ¯”è¾ƒä»·æ ¼: {row1[col]} vs {row2[col]}ï¼Œè·³è¿‡æ­¤æ¯”è¾ƒ")
                                # ä¿é™©èµ·è§ï¼Œæ ‡è®°ä¸ºæœ‰å·®å¼‚
                                diff_cols.append(col)
                    # å¯¹HSNåˆ—ç‰¹æ®Šå¤„ç†ï¼Œå¿½ç•¥æµ®ç‚¹æ•°å’Œæ•´æ•°çš„å·®å¼‚ï¼ˆå¦‚85423900.0å’Œ85423900ï¼‰
                    elif col == 'HSN':
                        if pd.notna(row1[col]) and pd.notna(row2[col]):
                            # å°†ä¸¤ä¸ªå€¼è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶å»é™¤å°æ•°ç‚¹åçš„é›¶
                            val1 = str(row1[col]).rstrip('0').rstrip('.') if '.' in str(row1[col]) else str(row1[col])
                            val2 = str(row2[col]).rstrip('0').rstrip('.') if '.' in str(row2[col]) else str(row2[col])

                            # è®°å½•HSNå€¼çš„æ¯”è¾ƒ
                            if val1 != val2:
                                logging.info(f"HSN difference found: {row1[col]} vs {row2[col]} (normalized: {val1} vs {val2})")
                                diff_cols.append(col)
                            else:
                                logging.debug(f"HSN values considered equal: {row1[col]} vs {row2[col]} (normalized: {val1} vs {val2})")
                    # å…¶ä»–åˆ—ä½¿ç”¨ç²¾ç¡®æ¯”å¯¹
                    elif row1[col] != row2[col]:
                        diff_cols.append(col)

                if diff_cols:
                    diff_count += 1
                    # åªåˆ›å»ºåŒ…å«IDå’Œæœ‰å·®å¼‚åˆ—çš„ä¿¡æ¯
                    diff_info = {'ID': id1}

                    # è®°å½•å·®å¼‚åˆ—
                    logging.info(f"ID {id1} çš„å·®å¼‚åˆ—: {diff_cols}")
                    
                    # æ·»åŠ è¯¦ç»†çš„BCDè°ƒè¯•ä¿¡æ¯
                    if 'BCD' in diff_cols:
                        logging.info(f"BCD difference detected for ID {id1}:")
                        logging.info(f"  - Invoice BCD: '{row1['BCD']}' (type: {type(row1['BCD'])})")
                        logging.info(f"  - Checklist BCD: '{row2['BCD']}' (type: {type(row2['BCD'])})")

                    # æŒ‰ç…§æœŸæœ›çš„åˆ—é¡ºåºå¤„ç†å·®å¼‚åˆ—
                    expected_columns = ['ID', 'P/N', 'Desc', 'HSN', 'BCD', 'SWS', 'IGST', 'Qty', 'Price']

                    # é¦–å…ˆå¤„ç†IDåˆ—
                    diff_info['ID'] = id1

                    # ç„¶åæŒ‰ç…§æœŸæœ›çš„é¡ºåºå¤„ç†å…¶ä»–åˆ—
                    for col in expected_columns:
                        if col != 'ID' and col in diff_cols:
                            # å¯¹æ•°å­—åˆ—ç‰¹æ®Šå¤„ç†æ˜¾ç¤ºæ ¼å¼ï¼Œå»é™¤å°æ•°ç‚¹åçš„é›¶
                            if col in ['HSN', 'BCD', 'SWS', 'IGST', 'Qty', 'Price']:
                                # å¤„ç†æ•°å€¼ï¼Œå»é™¤å°æ•°ç‚¹åçš„é›¶
                                val1 = str(row1[col]).rstrip('0').rstrip('.') if '.' in str(row1[col]) else str(row1[col])
                                val2 = str(row2[col]).rstrip('0').rstrip('.') if '.' in str(row2[col]) else str(row2[col])

                                # ç‰¹åˆ«ä¸ºBCDå­—æ®µæ·»åŠ è°ƒè¯•ä¿¡æ¯
                                if col == 'BCD':
                                    logging.info(f"Processing BCD difference for ID {id1}:")
                                    logging.info(f"  - Original values: '{row1[col]}' vs '{row2[col]}'")
                                    logging.info(f"  - Processed values: '{val1}' vs '{val2}'")

                                # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºå€¼/NaNå€¼
                                def is_empty_value(val):
                                    return (val.lower() in ['nan', 'none', ''] or
                                           val.strip() == '' or
                                           val == 'nan')

                                val1_is_empty = is_empty_value(val1)
                                val2_is_empty = is_empty_value(val2)

                                # ç‰¹åˆ«å¤„ç†ï¼šå¦‚æœå€¼ç›¸åŒï¼Œä½†åŸå§‹å€¼ä¸åŒï¼Œä»ç„¶è®°å½•å·®å¼‚
                                # è¿™è§£å†³äº†BCDå­—æ®µè¢«æ„å¤–è·³è¿‡çš„é—®é¢˜
                                if val1 == val2 and str(row1[col]) == str(row2[col]):
                                    # çœŸæ­£ç›¸åŒçš„å€¼æ‰è·³è¿‡
                                    if col == 'BCD':
                                        logging.info(f"BCD values are truly equal, skipping: {val1} == {val2}")
                                    continue

                                # å¤„ç†æ˜¾ç¤ºå€¼ï¼šç©ºå€¼æ˜¾ç¤ºä¸º "null"
                                display_val1 = 'null' if val1_is_empty else val1
                                display_val2 = 'null' if val2_is_empty else val2

                                # æ·»åŠ å·®å¼‚ä¿¡æ¯ï¼ˆæ ¼å¼ï¼šchecklistå€¼ -> invoiceå€¼ï¼‰
                                diff_info[f'{col}'] = f'{display_val2} -> {display_val1}'
                                
                                # ç‰¹åˆ«ä¸ºBCDå­—æ®µæ·»åŠ ç¡®è®¤ä¿¡æ¯
                                if col == 'BCD':
                                    logging.info(f"BCD difference added to report: '{display_val2} -> {display_val1}'")
                            else:
                                # è·³è¿‡ç›¸åŒå€¼çš„æ˜¾ç¤º
                                if row1[col] == row2[col]:
                                    continue

                                # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºå€¼/NaNå€¼
                                def is_empty_value_general(val):
                                    if pd.isna(val):
                                        return True
                                    val_str = str(val).strip().lower()
                                    return val_str in ['nan', 'none', ''] or val_str == ''

                                val1_is_empty = is_empty_value_general(row1[col])
                                val2_is_empty = is_empty_value_general(row2[col])

                                # åªæœ‰å½“ä¸¤ä¸ªå€¼éƒ½ä¸ºç©ºæ—¶æ‰è·³è¿‡
                                if val1_is_empty and val2_is_empty:
                                    continue

                                # è·³è¿‡ç›¸åŒå€¼çš„æ˜¾ç¤º (ä¾‹å¦‚ nan -> nan)
                                if str(row1[col]).lower() == str(row2[col]).lower():
                                    continue

                                # å¤„ç†æ˜¾ç¤ºå€¼ï¼šç©ºå€¼æ˜¾ç¤ºä¸º "null"
                                display_val1 = 'null' if val1_is_empty else str(row1[col])
                                display_val2 = 'null' if val2_is_empty else str(row2[col])

                                # æ·»åŠ å·®å¼‚ä¿¡æ¯ï¼ˆæ ¼å¼ï¼šchecklistå€¼ -> invoiceå€¼ï¼‰
                                diff_info[f'{col}'] = f'{display_val2} -> {display_val1}'

                    # åªæœ‰å½“diff_infoä¸­æœ‰é™¤äº†IDä»¥å¤–çš„å…¶ä»–åˆ—æ—¶æ‰æ·»åŠ åˆ°diff_data
                    if len(diff_info) > 1:
                        diff_data.append(diff_info)

        logging.info(f"Comparison complete. Found {match_count} matching IDs between files")
        logging.info(f"Found {diff_count} differences")

        if diff_data:
            diff_df = pd.DataFrame(diff_data)

            # åˆ—åå·²ç»æ˜¯ BCD å’Œ SWSï¼Œä¸éœ€è¦å†è¿›è¡Œæ˜ å°„

            # å®šä¹‰æœŸæœ›çš„åˆ—é¡ºåº (æŒ‰ç…§æŒ‡å®šé¡ºåº)
            expected_columns = ['ID', 'P/N', 'Desc', 'HSN', 'BCD', 'SWS', 'IGST', 'Qty', 'Price']

            # åªä¿ç•™å®é™…æœ‰æ•°æ®çš„åˆ—
            non_empty_columns = ['ID']  # IDåˆ—å§‹ç»ˆä¿ç•™

            # æ£€æŸ¥æ¯ä¸€åˆ—æ˜¯å¦æœ‰éç©º/énanæ•°æ®
            for col in diff_df.columns:
                if col != 'ID':  # è·³è¿‡IDåˆ—ï¼Œå› ä¸ºå·²ç»åŒ…å«
                    # æ£€æŸ¥åˆ—æ˜¯å¦åŒ…å«ä»»ä½•éç©ºå€¼
                    has_data = False
                    for val in diff_df[col]:
                        if pd.notna(val) and str(val).strip() != '' and str(val).lower() != 'nan' and str(val).lower() != 'none':
                            has_data = True
                            break

                    # ç‰¹åˆ«ä¸ºBCDåˆ—æ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯
                    if col == 'BCD':
                        logging.info(f"BCD column filtering check:")
                        logging.info(f"  - Column exists in diff_df: {col in diff_df.columns}")
                        logging.info(f"  - Has data: {has_data}")
                        if col in diff_df.columns:
                            logging.info(f"  - Column values: {diff_df[col].tolist()}")
                            logging.info(f"  - Non-null values: {[v for v in diff_df[col] if pd.notna(v) and str(v).strip() != '']}")

                    if has_data:
                        non_empty_columns.append(col)
                        logging.info(f"Column '{col}' added to non_empty_columns (has data)")
                    else:
                        logging.info(f"Column '{col}' skipped (no data found)")

            logging.info(f"Non-empty columns: {non_empty_columns}")

            # ä¸¥æ ¼æŒ‰ç…§æœŸæœ›çš„åˆ—é¡ºåºæ’åºï¼ŒåªåŒ…å«å­˜åœ¨çš„åˆ—
            ordered_columns = []
            for col in expected_columns:
                if col in diff_df.columns and (col == 'ID' or col in non_empty_columns):
                    ordered_columns.append(col)
                    logging.info(f"æ·»åŠ åˆ—åˆ°æœ€ç»ˆæŠ¥å‘Š: {col}")
                elif col in expected_columns:
                    # ç‰¹åˆ«ä¸ºBCDåˆ—æ·»åŠ è¯¦ç»†ä¿¡æ¯
                    if col == 'BCD':
                        logging.info(f"BCDåˆ—è¢«è·³è¿‡çš„åŸå› :")
                        logging.info(f"  - åœ¨diff_dfä¸­å­˜åœ¨: {col in diff_df.columns}")
                        logging.info(f"  - åœ¨non_empty_columnsä¸­: {col in non_empty_columns}")
                        if col in diff_df.columns:
                            logging.info(f"  - BCDåˆ—çš„æ‰€æœ‰å€¼: {diff_df[col].tolist()}")
                    logging.info(f"åˆ— {col} ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œè·³è¿‡")

            # ä¸å†æ·»åŠ å…¶ä»–åˆ—ï¼Œç¡®ä¿ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šé¡ºåº
            logging.info(f"æœ€ç»ˆåˆ—é¡ºåº: {ordered_columns}")

            # åº”ç”¨æ–°çš„åˆ—é¡ºåº
            diff_df = diff_df[ordered_columns]

            logging.info(f"Created difference DataFrame with shape: {diff_df.shape}")
            logging.info(f"Columns in final report: {diff_df.columns.tolist()}")

            # æ·»åŠ ç±»å‹è½¬æ¢ï¼Œè§£å†³Arrowåºåˆ—åŒ–é—®é¢˜
            for col in diff_df.columns:
                # æ‰€æœ‰åˆ—éƒ½è½¬ä¸ºå­—ç¬¦ä¸²ï¼ŒåŒ…æ‹¬ä¹‹å‰æ’é™¤çš„Item Name
                diff_df[col] = diff_df[col].astype(str)

            # é¢å¤–å¤„ç†ï¼šç¡®ä¿æ²¡æœ‰å­—ç¬¦ä¸²å½¢å¼çš„'nan'æˆ–'none'
            for col in diff_df.columns:
                diff_df[col] = diff_df[col].apply(
                    lambda x: '' if x.lower() in ['nan', 'none'] else x
                )

            # ç‰¹åˆ«å¤„ç†å¯èƒ½å­˜åœ¨çš„Dutyå’ŒWelfareåˆ—ï¼ˆæ—§åç§°ï¼‰
            for old_col in ['Duty', 'Welfare']:
                if old_col in diff_df.columns:
                    diff_df[old_col] = diff_df[old_col].astype(str)
                    logging.info(f"è½¬æ¢äº†å·®å¼‚æŠ¥å‘Šä¸­çš„æ—§åˆ—å {old_col} ä¸ºå­—ç¬¦ä¸²ç±»å‹")

            return diff_df
        else:
            logging.info("No differences found between files")
            return pd.DataFrame()

    except Exception as e:
        error_msg = f"æ¯”è¾ƒæ–‡ä»¶å¤±è´¥: {str(e)}"
        logging.error(error_msg)
        logging.exception("Exception details:")
        st.error(error_msg)
        return pd.DataFrame()

def generate_email_draft(diff_report_df):
    """
    æ ¹æ®å·®å¼‚æŠ¥å‘Šç”Ÿæˆé‚®ä»¶è‰ç¨¿å†…å®¹
    """
    logging.info("ç”Ÿæˆé‚®ä»¶è‰ç¨¿å¼€å§‹")

    if diff_report_df.empty:
        logging.info("æ²¡æœ‰å·®å¼‚æ•°æ®ï¼Œæ— éœ€ç”Ÿæˆé‚®ä»¶è‰ç¨¿")
        return None

    try:
        email_body_lines = []
        email_body_lines.append("-------------------------------------------------------")
        email_body_lines.append("Hello ,")
        email_body_lines.append("")
        email_body_lines.append("Please revise the checklist as below:")

        # éå†æ¯ä¸€è¡Œå·®å¼‚æ•°æ®
        for _, row in diff_report_df.iterrows():
            invoice_id = row.get('ID', 'Unknown')

            # æ„å»ºé‚®ä»¶å†…å®¹è¡Œ
            corrections = []

            # æ£€æŸ¥å„ä¸ªå­—æ®µçš„å·®å¼‚å¹¶æ„å»ºä¿®æ­£ä¿¡æ¯
            if 'HSN' in row and pd.notna(row['HSN']) and str(row['HSN']).strip():
                # æå–æ­£ç¡®å€¼ï¼ˆæ ¼å¼ï¼šchecklistå€¼ -> invoiceå€¼ï¼Œæˆ‘ä»¬éœ€è¦invoiceå€¼ï¼‰
                hsn_correct = str(row['HSN']).split(' -> ')[1] if ' -> ' in str(row['HSN']) else str(row['HSN'])
                if hsn_correct != 'null':  # åªæœ‰å½“æ­£ç¡®å€¼ä¸æ˜¯nullæ—¶æ‰æ·»åŠ 
                    corrections.append(f"HSN {hsn_correct}")

            if 'BCD' in row and pd.notna(row['BCD']) and str(row['BCD']).strip():
                # æå–æ­£ç¡®å€¼ï¼ˆæ ¼å¼ï¼šchecklistå€¼ -> invoiceå€¼ï¼Œæˆ‘ä»¬éœ€è¦invoiceå€¼ï¼‰
                bcd_correct = str(row['BCD']).split(' -> ')[1] if ' -> ' in str(row['BCD']) else str(row['BCD'])
                if bcd_correct != 'null':  # åªæœ‰å½“æ­£ç¡®å€¼ä¸æ˜¯nullæ—¶æ‰æ·»åŠ 
                    corrections.append(f"BCD is {bcd_correct}")

            if 'SWS' in row and pd.notna(row['SWS']) and str(row['SWS']).strip():
                # æå–æ­£ç¡®å€¼ï¼ˆæ ¼å¼ï¼šchecklistå€¼ -> invoiceå€¼ï¼Œæˆ‘ä»¬éœ€è¦invoiceå€¼ï¼‰
                sws_correct = str(row['SWS']).split(' -> ')[1] if ' -> ' in str(row['SWS']) else str(row['SWS'])
                if sws_correct != 'null':  # åªæœ‰å½“æ­£ç¡®å€¼ä¸æ˜¯nullæ—¶æ‰æ·»åŠ 
                    corrections.append(f"SWS is {sws_correct}")

            if 'IGST' in row and pd.notna(row['IGST']) and str(row['IGST']).strip():
                # æå–æ­£ç¡®å€¼ï¼ˆæ ¼å¼ï¼šchecklistå€¼ -> invoiceå€¼ï¼Œæˆ‘ä»¬éœ€è¦invoiceå€¼ï¼‰
                hgst_correct = str(row['IGST']).split(' -> ')[1] if ' -> ' in str(row['IGST']) else str(row['IGST'])
                if hgst_correct != 'null':  # åªæœ‰å½“æ­£ç¡®å€¼ä¸æ˜¯nullæ—¶æ‰æ·»åŠ 
                    corrections.append(f"HGST is {hgst_correct}")

            if 'Qty' in row and pd.notna(row['Qty']) and str(row['Qty']).strip():
                # æå–æ­£ç¡®å€¼ï¼ˆæ ¼å¼ï¼šchecklistå€¼ -> invoiceå€¼ï¼Œæˆ‘ä»¬éœ€è¦invoiceå€¼ï¼‰
                qty_correct = str(row['Qty']).split(' -> ')[1] if ' -> ' in str(row['Qty']) else str(row['Qty'])
                if qty_correct != 'null':  # åªæœ‰å½“æ­£ç¡®å€¼ä¸æ˜¯nullæ—¶æ‰æ·»åŠ 
                    corrections.append(f"Qty is {qty_correct}")

            if 'Price' in row and pd.notna(row['Price']) and str(row['Price']).strip():
                # æå–æ­£ç¡®å€¼ï¼ˆæ ¼å¼ï¼šchecklistå€¼ -> invoiceå€¼ï¼Œæˆ‘ä»¬éœ€è¦invoiceå€¼ï¼‰
                price_correct = str(row['Price']).split(' -> ')[1] if ' -> ' in str(row['Price']) else str(row['Price'])
                if price_correct != 'null':  # åªæœ‰å½“æ­£ç¡®å€¼ä¸æ˜¯nullæ—¶æ‰æ·»åŠ 
                    corrections.append(f"Price is {price_correct}")

            if 'Desc' in row and pd.notna(row['Desc']) and str(row['Desc']).strip():
                # æå–æ­£ç¡®å€¼ï¼ˆæ ¼å¼ï¼šchecklistå€¼ -> invoiceå€¼ï¼Œæˆ‘ä»¬éœ€è¦invoiceå€¼ï¼‰
                desc_correct = str(row['Desc']).split(' -> ')[1] if ' -> ' in str(row['Desc']) else str(row['Desc'])
                if desc_correct != 'null':  # åªæœ‰å½“æ­£ç¡®å€¼ä¸æ˜¯nullæ—¶æ‰æ·»åŠ 
                    corrections.append(f"Description is {desc_correct}")

            if corrections:
                email_line = f"Invoice {invoice_id} use {' '.join(corrections)}ã€‚"
                email_body_lines.append(email_line)

        email_body_lines.append("")
        email_body_lines.append("Thank you!")
        email_body_lines.append("-------------------------------------------------------")

        email_content = "\n".join(email_body_lines)
        logging.info(f"é‚®ä»¶è‰ç¨¿ç”Ÿæˆå®Œæˆï¼Œå…±{len(diff_report_df)}æ¡å·®å¼‚è®°å½•")

        return email_content

    except Exception as e:
        error_msg = f"ç”Ÿæˆé‚®ä»¶è‰ç¨¿å¤±è´¥: {str(e)}"
        logging.error(error_msg)
        logging.exception("Exception details:")
        return None

def open_email_client(email_content, subject="Checklist Revision Required"):
    """
    æ‰“å¼€é»˜è®¤é‚®ä»¶å®¢æˆ·ç«¯å¹¶å¡«å……é‚®ä»¶å†…å®¹
    """
    try:
        # URLç¼–ç é‚®ä»¶å†…å®¹
        encoded_subject = urllib.parse.quote(subject)
        encoded_body = urllib.parse.quote(email_content)

        # æ„å»ºmailtoé“¾æ¥
        mailto_url = f"mailto:?subject={encoded_subject}&body={encoded_body}"

        # æ‰“å¼€é‚®ä»¶å®¢æˆ·ç«¯
        webbrowser.open(mailto_url)
        logging.info("å·²æ‰“å¼€é»˜è®¤é‚®ä»¶å®¢æˆ·ç«¯")
        return True

    except Exception as e:
        error_msg = f"æ‰“å¼€é‚®ä»¶å®¢æˆ·ç«¯å¤±è´¥: {str(e)}"
        logging.error(error_msg)
        logging.exception("Exception details:")
        return False

def safe_display_dataframe(df, container_width=True):
    """
    å®‰å…¨æ˜¾ç¤º DataFrameï¼Œé¿å… PyArrow åºåˆ—åŒ–é”™è¯¯
    """
    try:
        # åˆ›å»ºå‰¯æœ¬å¹¶è½¬æ¢æ‰€æœ‰åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
        display_df = df.copy()
        for col in display_df.columns:
            display_df[col] = display_df[col].astype(str)
        
        # å¤„ç† NaN å€¼
        display_df = display_df.fillna('')
        
        # æ˜¾ç¤º DataFrame
        st.dataframe(display_df, use_container_width=container_width)
        return True
        
    except Exception as display_error:
        # å¦‚æœè¿˜æ˜¯æœ‰é—®é¢˜ï¼Œæ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
        st.warning(f"æ•°æ®æ˜¾ç¤ºé‡åˆ°é—®é¢˜ï¼Œæ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯: {str(display_error)}")
        st.write(f"æ•°æ®å½¢çŠ¶: {df.shape}")
        st.write(f"åˆ—å: {df.columns.tolist()}")
        
        # å°è¯•æ˜¾ç¤ºå‰å‡ è¡Œçš„æ–‡æœ¬ç‰ˆæœ¬
        if not df.empty:
            st.write("å‰5è¡Œæ•°æ®:")
            try:
                st.text(df.head().to_string())
            except Exception as text_error:
                st.error(f"æ— æ³•æ˜¾ç¤ºæ•°æ®: {str(text_error)}")
        return False

# File Upload Tab
with tab1:
    st.markdown("<h2 class='sub-header'>æ–‡ä»¶ä¸Šä¼ ä¸è®¾ç½®</h2>", unsafe_allow_html=True)

    # åˆ›å»ºä¸¤è¡Œå¸ƒå±€ï¼šç¬¬ä¸€è¡Œæ˜¯æ–‡ä»¶ä¸Šä¼ ï¼Œç¬¬äºŒè¡Œæ˜¯è®¾ç½®å’Œå¤„ç†

    # ç¬¬ä¸€è¡Œï¼šæ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    st.markdown("### ğŸ“ æ–‡ä»¶ä¸Šä¼ ")
    col1, col2, col3 = st.columns([1, 1, 1], gap="medium")

    with col1:
        st.markdown("""
        <div class='upload-section'>
            <h3>ç¨ç‡æ–‡ä»¶</h3>
        </div>
        """, unsafe_allow_html=True)
        duty_rate_file = st.file_uploader("ä¸Šä¼ ç¨ç‡æ–‡ä»¶", type=["xlsx"], key="duty_rate")
        if duty_rate_file is not None:
            try:
                # ä½¿ç”¨å®‰å…¨ä¿å­˜å‡½æ•°
                safe_path, safe_name = safe_save_uploaded_file(duty_rate_file, os.path.join("input", "duty_rate.xlsx"))
                st.success(f"âœ… å·²ä¸Šä¼ : {safe_name}")
                # æ›´æ–°session state
                st.session_state.duty_rate_uploaded = True
                st.session_state.duty_rate_path = safe_path
            except Exception as e:
                st.error(f"âŒ ä¸Šä¼ å¤±è´¥: {str(e)}")
                st.session_state.duty_rate_uploaded = False
        else:
            st.info("ğŸ“ è¯·ä¸Šä¼ ç¨ç‡æ–‡ä»¶")
            st.session_state.duty_rate_uploaded = False

    with col2:
        st.markdown("""
        <div class='upload-section'>
            <h3>æ ¸å¯¹æ¸…å•</h3>
        </div>
        """, unsafe_allow_html=True)
        checklist_file = st.file_uploader("ä¸Šä¼ æ ¸å¯¹æ¸…å•", type=["xlsx"], key="checklist")
        if checklist_file is not None:
            try:
                # ä½¿ç”¨å®‰å…¨ä¿å­˜å‡½æ•°
                safe_path, safe_name = safe_save_uploaded_file(checklist_file, os.path.join("input", "processing_checklist.xlsx"))
                st.success(f"âœ… å·²ä¸Šä¼ : {safe_name}")
                # æ›´æ–°session state
                st.session_state.checklist_uploaded = True
                st.session_state.checklist_path = safe_path
            except Exception as e:
                st.error(f"âŒ ä¸Šä¼ å¤±è´¥: {str(e)}")
                st.session_state.checklist_uploaded = False
        else:
            st.info("ğŸ“ è¯·ä¸Šä¼ æ ¸å¯¹æ¸…å•")
            st.session_state.checklist_uploaded = False

    with col3:
        st.markdown("""
        <div class='upload-section'>
            <h3>å‘ç¥¨æ–‡ä»¶</h3>
        </div>
        """, unsafe_allow_html=True)
        invoices_file = st.file_uploader("ä¸Šä¼ å‘ç¥¨æ–‡ä»¶", type=["xlsx"], key="invoices")
        if invoices_file is not None:
            try:
                # ä½¿ç”¨å®‰å…¨ä¿å­˜å‡½æ•°
                safe_path, safe_name = safe_save_uploaded_file(invoices_file, os.path.join("input", "processing_invoices.xlsx"))
                st.success(f"âœ… å·²ä¸Šä¼ : {safe_name}")
                # æ›´æ–°session state
                st.session_state.invoices_uploaded = True
                st.session_state.invoices_path = safe_path
            except Exception as e:
                st.error(f"âŒ ä¸Šä¼ å¤±è´¥: {str(e)}")
                st.session_state.invoices_uploaded = False
        else:
            st.info("ğŸ“ è¯·ä¸Šä¼ å‘ç¥¨æ–‡ä»¶")
            st.session_state.invoices_uploaded = False

    # æ·»åŠ åˆ†éš”çº¿
    st.markdown("---")

    # ç¬¬äºŒè¡Œï¼šè®¾ç½®å’Œå¤„ç†åŒºåŸŸ
    col_settings, col_process, col_help = st.columns([2, 1, 2], gap="large")

    with col_settings:
        st.markdown("""
        <div class='settings-section'>
            <h3>âš™ï¸ å¤„ç†è®¾ç½®</h3>
        </div>
        """, unsafe_allow_html=True)
        price_tolerance = st.slider("ä»·æ ¼æ¯”å¯¹è¯¯å·®èŒƒå›´ (%)", min_value=0.1, max_value=5.0, value=1.1, step=0.1)
        st.caption(f"å½“å‰è®¾ç½®: ä»·æ ¼å·®å¼‚è¶…è¿‡ {price_tolerance}% å°†è¢«æ ‡è®°")

    with col_process:
        st.markdown("""
        <div class='process-section'>
            <h3>ğŸš€ å¼€å§‹å¤„ç†</h3>
        </div>
        """, unsafe_allow_html=True)
        process_button = st.button("å¼€å§‹å¤„ç†", type="primary", use_container_width=True)

        # æ˜¾ç¤ºå¤„ç†çŠ¶æ€
        if st.session_state.get('duty_rate_uploaded', False) and \
           st.session_state.get('checklist_uploaded', False) and \
           st.session_state.get('invoices_uploaded', False):
            st.success("âœ… æ‰€æœ‰æ–‡ä»¶å·²å°±ç»ª")
        else:
            st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ‰€æœ‰æ–‡ä»¶")

    with col_help:
        st.markdown("""
        <div class='help-section'>
            <h3>ğŸ’¡ ä½¿ç”¨è¯´æ˜</h3>
        </div>
        """, unsafe_allow_html=True)
        with st.expander("æŸ¥çœ‹è¯¦ç»†æ­¥éª¤", expanded=False):
            st.markdown("""
            **æ“ä½œæ­¥éª¤ï¼š**
            1. ğŸ“„ ä¸Šä¼ ç¨ç‡æ–‡ä»¶ (duty_rate.xlsx)
            2. ğŸ“‹ ä¸Šä¼ æ ¸å¯¹æ¸…å• (processing_checklist.xlsx)
            3. ğŸ§¾ ä¸Šä¼ å‘ç¥¨æ–‡ä»¶ (processing_invoices*.xlsx)
            4. âš™ï¸ è°ƒæ•´ä»·æ ¼æ¯”å¯¹è¯¯å·®èŒƒå›´ï¼ˆå¯é€‰ï¼‰
            5. ğŸš€ ç‚¹å‡»"å¼€å§‹å¤„ç†"æŒ‰é’®
            6. ğŸ“Š åœ¨å…¶ä»–æ ‡ç­¾é¡µæŸ¥çœ‹å¤„ç†ç»“æœ

            **æ³¨æ„äº‹é¡¹ï¼š**
            - ç¡®ä¿æ–‡ä»¶æ ¼å¼ä¸º .xlsx
            - æ–‡ä»¶å¤§å°é™åˆ¶ä¸º 200MB
            - å¤„ç†æ—¶é—´å–å†³äºæ•°æ®é‡å¤§å°
            """)

# Data Preview Tab
with tab2:
    st.markdown("<h2 class='sub-header'>æ•°æ®é¢„è§ˆ</h2>", unsafe_allow_html=True)

    preview_tabs = st.tabs(["ç¨ç‡æ•°æ®", "æ ¸å¯¹æ¸…å•", "å‘ç¥¨æ•°æ®"])

    with preview_tabs[0]:
        if duty_rate_file is not None:
            try:
                duty_df = pd.read_excel(duty_rate_file)
                safe_display_dataframe(duty_df)
            except Exception as e:
                st.error(f"æ— æ³•é¢„è§ˆç¨ç‡æ–‡ä»¶: {str(e)}")
        else:
            st.info("è¯·å…ˆä¸Šä¼ ç¨ç‡æ–‡ä»¶")

    with preview_tabs[1]:
        if checklist_file is not None:
            try:
                checklist_df = pd.read_excel(checklist_file, skiprows=3)
                safe_display_dataframe(checklist_df)
            except Exception as e:
                st.error(f"æ— æ³•é¢„è§ˆæ ¸å¯¹æ¸…å•: {str(e)}")
        else:
            st.info("è¯·å…ˆä¸Šä¼ æ ¸å¯¹æ¸…å•")

    with preview_tabs[2]:
        if invoices_file is not None:
            try:
                excel_file = pd.ExcelFile(invoices_file)
                sheet_names = excel_file.sheet_names[1:]  # Skip the first sheet

                if sheet_names:
                    selected_sheet = st.selectbox("é€‰æ‹©å‘ç¥¨å·¥ä½œè¡¨", sheet_names)
                    invoices_df = pd.read_excel(invoices_file, sheet_name=selected_sheet)
                    safe_display_dataframe(invoices_df)
                else:
                    st.warning("å‘ç¥¨æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°å·¥ä½œè¡¨")
            except Exception as e:
                st.error(f"æ— æ³•é¢„è§ˆå‘ç¥¨æ–‡ä»¶: {str(e)}")
        else:
            st.info("è¯·å…ˆä¸Šä¼ å‘ç¥¨æ–‡ä»¶")

# Process data when button is clicked
if process_button:
    logging.info("Process button clicked")
    if duty_rate_file is None or checklist_file is None or invoices_file is None:
        missing_files = []
        if duty_rate_file is None:
            missing_files.append("ç¨ç‡æ–‡ä»¶")
        if checklist_file is None:
            missing_files.append("æ ¸å¯¹æ¸…å•")
        if invoices_file is None:
            missing_files.append("å‘ç¥¨æ–‡ä»¶")
        error_msg = f"è¯·å…ˆä¸Šä¼ æ‰€æœ‰å¿…è¦çš„æ–‡ä»¶: {', '.join(missing_files)}"
        logging.error(error_msg)
        st.error(error_msg)
    else:
        logging.info("Starting data processing workflow")
        with st.spinner("æ­£åœ¨å¤„ç†æ•°æ®..."):
            try:
                # ä½¿ç”¨session stateä¸­ä¿å­˜çš„å®é™…æ–‡ä»¶è·¯å¾„
                duty_rate_path = st.session_state.get('duty_rate_path', os.path.join("input", "duty_rate.xlsx"))
                invoices_path = st.session_state.get('invoices_path', os.path.join("input", "processing_invoices.xlsx"))
                checklist_path = st.session_state.get('checklist_path', os.path.join("input", "processing_checklist.xlsx"))

                logging.info(f"Input files: duty_rate={duty_rate_path}, invoices={invoices_path}, checklist={checklist_path}")
                logging.info(f"Price tolerance: {price_tolerance}%")
                
                # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not os.path.exists(duty_rate_path):
                    raise FileNotFoundError(f"ç¨ç‡æ–‡ä»¶ä¸å­˜åœ¨: {duty_rate_path}")
                if not os.path.exists(invoices_path):
                    raise FileNotFoundError(f"å‘ç¥¨æ–‡ä»¶ä¸å­˜åœ¨: {invoices_path}")
                if not os.path.exists(checklist_path):
                    raise FileNotFoundError(f"æ ¸å¯¹æ¸…å•æ–‡ä»¶ä¸å­˜åœ¨: {checklist_path}")

                # Process duty rates
                logging.info("Step 1: Processing duty rates")
                duty_rates, duty_df = get_duty_rates(duty_rate_path)

                # Process invoices
                logging.info("Step 2: Processing invoices")
                processed_invoices, new_items = process_invoice_file(invoices_path, duty_rates)

                # Process checklist
                logging.info("Step 3: Processing checklist")
                processed_checklist = process_checklist(checklist_path)

                # Compare the processed files
                logging.info("Step 4: Comparing processed files")
                diff_report = compare_excels(processed_invoices, processed_checklist, price_tolerance)

                # Save the processed files
                logging.info("Step 5: Saving output files")
                processed_invoices_path = os.path.join("output", "processed_invoices.xlsx")
                processed_checklist_path = os.path.join("output", "processed_checklist.xlsx")
                processed_report_path = os.path.join("output", "processed_report.xlsx")

                try:
                    # Save the diff report if it's not empty
                    if not diff_report.empty:
                        # åœ¨å¯¼å‡ºå‰ç¡®ä¿æ²¡æœ‰NaNå€¼
                        export_diff_report = diff_report.copy()

                        # å°†æ‰€æœ‰NaNå€¼æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
                        export_diff_report = export_diff_report.fillna('')

                        # é¢å¤–å¤„ç†ï¼šç¡®ä¿æ²¡æœ‰å­—ç¬¦ä¸²å½¢å¼çš„'nan'æˆ–'none'
                        for col in export_diff_report.columns:
                            export_diff_report[col] = export_diff_report[col].apply(
                                lambda x: '' if isinstance(x, str) and x.lower() in ['nan', 'none'] else x
                            )

                        # ä½¿ç”¨xlsxwriterå¼•æ“ä¿å­˜ï¼Œä»¥ä¾¿è®¾ç½®åˆ—å®½
                        with pd.ExcelWriter(processed_report_path, engine='xlsxwriter') as writer:
                            export_diff_report.to_excel(writer, index=False, sheet_name='å·®å¼‚æŠ¥å‘Š')

                            # è·å–å·¥ä½œè¡¨
                            worksheet = writer.sheets['å·®å¼‚æŠ¥å‘Š']

                            # è®¾ç½®åˆ—å®½ - é™¤äº†DESCå­—æ®µå¤–çš„æ‰€æœ‰åˆ—éƒ½å®½ä¸€å€
                            for i, col in enumerate(export_diff_report.columns):
                                worksheet.set_column(i, i, 22)

                        logging.info(f"Saved diff report to {processed_report_path} with custom column widths")

                        # åˆ›å»ºç”¨äºè‡ªåŠ¨ä¸‹è½½çš„BytesIOå¯¹è±¡
                        report_buffer = BytesIO()
                        with pd.ExcelWriter(report_buffer, engine='xlsxwriter') as writer:
                            # ä½¿ç”¨å·²ç»å¤„ç†è¿‡NaNçš„export_diff_report
                            export_diff_report.to_excel(writer, index=False, sheet_name='å·®å¼‚æŠ¥å‘Š')

                            # è·å–å·¥ä½œè¡¨
                            worksheet = writer.sheets['å·®å¼‚æŠ¥å‘Š']

                            # è®¾ç½®åˆ—å®½ - é™¤äº†DESCå­—æ®µå¤–çš„æ‰€æœ‰åˆ—éƒ½å®½ä¸€å€
                            for i, col in enumerate(export_diff_report.columns):
                                worksheet.set_column(i, i, 22)

                        report_buffer.seek(0)

                        # è®¾ç½®ä¼šè¯çŠ¶æ€å˜é‡ï¼Œç”¨äºè‡ªåŠ¨ä¸‹è½½
                        st.session_state.auto_download_report = report_buffer
                        st.session_state.show_download_button = True

                        # ç”Ÿæˆé‚®ä»¶è‰ç¨¿
                        logging.info("Step 6: Generating email draft")
                        email_content = generate_email_draft(diff_report)
                        if email_content:
                            st.session_state.email_draft_content = email_content
                            st.session_state.show_email_button = True
                            logging.info("é‚®ä»¶è‰ç¨¿ç”ŸæˆæˆåŠŸ")
                        else:
                            st.session_state.show_email_button = False
                    else:
                        logging.info("No differences found, creating empty diff report")
                        # å³ä½¿æ²¡æœ‰å·®å¼‚ï¼Œä¹Ÿåˆ›å»ºä¸€ä¸ªç©ºçš„æŠ¥å‘Šæ–‡ä»¶
                        empty_diff_df = pd.DataFrame({'æ¶ˆæ¯': ['æ²¡æœ‰å‘ç°å·®å¼‚']})
                        empty_diff_df.to_excel(processed_report_path, index=False)

                        # ä»ç„¶æ˜¾ç¤ºä¸‹è½½æŒ‰é’®ï¼Œè®©ç”¨æˆ·å¯ä»¥ä¸‹è½½ç©ºæŠ¥å‘Š
                        st.session_state.show_download_button = True
                        st.session_state.show_email_button = False

                        # åˆ›å»ºç©ºçš„ä¸‹è½½ç¼“å†²åŒº
                        report_buffer = BytesIO()
                        empty_diff_df.to_excel(report_buffer, index=False)
                        report_buffer.seek(0)
                        st.session_state.auto_download_report = report_buffer
                except Exception as e:
                    logging.error(f"Error saving output files: {str(e)}")
                    logging.exception("Exception details:")
                    st.error(f"ä¿å­˜è¾“å‡ºæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")

                # Save new items if any
                if not new_items.empty:
                    logging.info(f"Found {len(new_items)} new items, saving to added_new_items.xlsx")
                    # Create a new Excel file with the original data
                    new_items_path = os.path.join("output", "added_new_items.xlsx")
                    try:
                        with pd.ExcelWriter(new_items_path, engine='xlsxwriter') as writer:
                            # æ£€æŸ¥duty_dfæ˜¯å¦ä¸ºNone
                            if duty_df is not None:
                                duty_df.to_excel(writer, index=False, sheet_name='CCTV')

                                # è·å–CCTVå·¥ä½œè¡¨å¹¶è®¾ç½®åˆ—å®½
                                worksheet_cctv = writer.sheets['CCTV']
                                for i, col in enumerate(duty_df.columns):
                                    # è®¡ç®—åˆ—çš„å®½åº¦
                                    max_len = max(
                                        duty_df[col].astype(str).map(len).max(),  # æœ€é•¿å†…å®¹
                                        len(str(col))  # åˆ—åé•¿åº¦
                                    ) + 2  # æ·»åŠ ä¸€äº›é¢å¤–ç©ºé—´

                                    # é™¤äº†æè¿°ç±»åˆ—å¤–çš„æ‰€æœ‰åˆ—éƒ½å®½ä¸€å€
                                    if col != 'Item Name' and 'Desc' not in col:
                                        max_len = max_len * 1.2

                                    # è®¾ç½®åˆ—å®½
                                    worksheet_cctv.set_column(i, i, max_len)
                            else:
                                logging.warning("duty_df is None, skipping CCTV sheet creation")

                            # Add new descriptions to a new sheet with required columns
                            required_columns = ['å‘ç¥¨åŠé¡¹å·', 'Item Name', 'Final BCD', 'Final SWS', 'Final IGST', 'HSN1']
                            new_items[required_columns].to_excel(writer, sheet_name='newDutyRate', index=False)

                            # è·å–newDutyRateå·¥ä½œè¡¨å¹¶è®¾ç½®åˆ—å®½
                            worksheet_new = writer.sheets['newDutyRate']
                            for i, col in enumerate(required_columns):
                                # è®¡ç®—åˆ—çš„å®½åº¦
                                max_len = max(
                                    new_items[col].astype(str).map(len).max(),  # æœ€é•¿å†…å®¹
                                    len(str(col))  # åˆ—åé•¿åº¦
                                ) + 2  # æ·»åŠ ä¸€äº›é¢å¤–ç©ºé—´

                                # é™¤äº†æè¿°ç±»åˆ—å¤–çš„æ‰€æœ‰åˆ—éƒ½å®½ä¸€å€
                                if col != 'Item Name':
                                    max_len = max_len * 1.2

                                # è®¾ç½®åˆ—å®½
                                worksheet_new.set_column(i, i, max_len)

                        logging.info(f"Saved new items to {new_items_path} with custom column widths")
                    except Exception as e:
                        logging.error(f"Error saving new items: {str(e)}")
                        logging.exception("Exception details:")
                        st.error(f"ä¿å­˜æ–°é¡¹ç›®æ—¶å‡ºé”™: {str(e)}")

                logging.info("Data processing completed successfully")
                st.success("æ•°æ®å¤„ç†å®Œæˆï¼")
            except Exception as e:
                error_msg = f"å¤„ç†æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
                logging.error(error_msg)
                logging.exception("Exception details:")
                st.error(error_msg)

# Results Tab
with tab3:
    st.markdown("<h2 class='sub-header'>å¤„ç†ç»“æœ</h2>", unsafe_allow_html=True)

    results_tabs = st.tabs(["å¤„ç†åçš„å‘ç¥¨", "å¤„ç†åçš„æ ¸å¯¹æ¸…å•", "æ–°å¢ç¨ç‡é¡¹"])

    with results_tabs[0]:
        processed_invoices_path = os.path.join("output", "processed_invoices.xlsx")
        if os.path.exists(processed_invoices_path):
            try:
                processed_invoices_df = pd.read_excel(processed_invoices_path)
                safe_display_dataframe(processed_invoices_df)

                # Download button
                with open(processed_invoices_path, "rb") as file:
                    st.download_button(
                        label="ä¸‹è½½å¤„ç†åçš„å‘ç¥¨",
                        data=file,
                        file_name="processed_invoices.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )
            except Exception as e:
                st.error(f"æ— æ³•åŠ è½½å¤„ç†åçš„å‘ç¥¨: {str(e)}")
        else:
            st.info("è¯·å…ˆå¤„ç†æ•°æ®")

    with results_tabs[1]:
        processed_checklist_path = os.path.join("output", "processed_checklist.xlsx")
        if os.path.exists(processed_checklist_path):
            try:
                processed_checklist_df = pd.read_excel(processed_checklist_path)
                safe_display_dataframe(processed_checklist_df)

                # Download button
                with open(processed_checklist_path, "rb") as file:
                    st.download_button(
                        label="ä¸‹è½½å¤„ç†åçš„æ ¸å¯¹æ¸…å•",
                        data=file,
                        file_name="processed_checklist.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )
            except Exception as e:
                st.error(f"æ— æ³•åŠ è½½å¤„ç†åçš„æ ¸å¯¹æ¸…å•: {str(e)}")
        else:
            st.info("è¯·å…ˆå¤„ç†æ•°æ®")

    with results_tabs[2]:
        new_items_path = os.path.join("output", "added_new_items.xlsx")
        if os.path.exists(new_items_path):
            try:
                excel_file = pd.ExcelFile(new_items_path)
                sheet_names = excel_file.sheet_names

                if 'newDutyRate' in sheet_names:
                    new_items_df = pd.read_excel(new_items_path, sheet_name='newDutyRate')
                    if not new_items_df.empty:
                        safe_display_dataframe(new_items_df)

                        # Download button
                        with open(new_items_path, "rb") as file:
                            st.download_button(
                                label="ä¸‹è½½æ–°å¢ç¨ç‡é¡¹",
                                data=file,
                                file_name="added_new_items.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                            )
                    else:
                        st.info("æ²¡æœ‰å‘ç°æ–°å¢ç¨ç‡é¡¹")
                else:
                    st.info("æ²¡æœ‰å‘ç°æ–°å¢ç¨ç‡é¡¹")
            except Exception as e:
                st.error(f"æ— æ³•åŠ è½½æ–°å¢ç¨ç‡é¡¹: {str(e)}")
        else:
            st.info("æ²¡æœ‰å‘ç°æ–°å¢ç¨ç‡é¡¹æˆ–è¯·å…ˆå¤„ç†æ•°æ®")

# Diff Report Tab
with tab4:
    st.markdown("<h2 class='sub-header'>å·®å¼‚æŠ¥å‘Š</h2>", unsafe_allow_html=True)

    diff_report_path = os.path.join("output", "processed_report.xlsx")
    if os.path.exists(diff_report_path):
        try:
            diff_report_df = pd.read_excel(diff_report_path)

            if not diff_report_df.empty:
                safe_display_dataframe(diff_report_df)

                col1, col2 = st.columns(2)

                with col1:
                    # ä¿®æ”¹ä¸‹è½½æŒ‰é’®ï¼Œæ·»åŠ use_container_width=Trueæ¥é˜²æ­¢è·³è½¬
                    with open(diff_report_path, "rb") as file:
                        st.download_button(
                            label="ğŸ“¥ ä¸‹è½½å·®å¼‚æŠ¥å‘Š",
                            data=file,
                            file_name="processed_report.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            use_container_width=True,
                            help="ä¸‹è½½å·®å¼‚æ¯”å¯¹æŠ¥å‘Šæ–‡ä»¶"
                        )

                with col2:
                    # æ”¹è¿›é‚®ä»¶è‰ç¨¿æŒ‰é’®ï¼Œä½¿ç”¨æ›´å¥½çš„çŠ¶æ€ç®¡ç†
                    # åˆå§‹åŒ–é‚®ä»¶ç”ŸæˆçŠ¶æ€
                    if 'email_button_clicked' not in st.session_state:
                        st.session_state.email_button_clicked = False

                    if st.button("ğŸ“§ ç”Ÿæˆé€šçŸ¥é‚®ä»¶è‰ç¨¿", type="secondary", key="generate_email_button", use_container_width=True):
                        # è®¾ç½®æŒ‰é’®ç‚¹å‡»çŠ¶æ€
                        st.session_state.email_button_clicked = True

                        email_content = generate_email_draft(diff_report_df)
                        if email_content:
                            # æ˜¾ç¤ºé‚®ä»¶å†…å®¹é¢„è§ˆ
                            st.session_state.email_draft_content = email_content

                            # ä½¿ç”¨å®¹å™¨æ¥æ˜¾ç¤ºçŠ¶æ€æ¶ˆæ¯ï¼Œé¿å…é¡µé¢è·³è½¬
                            success_container = st.container()
                            with success_container:
                                st.success("âœ… é‚®ä»¶è‰ç¨¿å·²ç”Ÿæˆï¼")

                            # å°è¯•æ‰“å¼€é‚®ä»¶å®¢æˆ·ç«¯
                            if open_email_client(email_content):
                                info_container = st.container()
                                with info_container:
                                    st.info("ğŸ“§ å·²å°è¯•æ‰“å¼€é»˜è®¤é‚®ä»¶å®¢æˆ·ç«¯")
                            else:
                                warning_container = st.container()
                                with warning_container:
                                    st.warning("âš ï¸ æ— æ³•æ‰“å¼€é‚®ä»¶å®¢æˆ·ç«¯ï¼Œè¯·æŸ¥çœ‹ä¸‹æ–¹é‚®ä»¶å†…å®¹")
                        else:
                            error_container = st.container()
                            with error_container:
                                st.error("âŒ ç”Ÿæˆé‚®ä»¶è‰ç¨¿å¤±è´¥")

                    # å¦‚æœæŒ‰é’®è¢«ç‚¹å‡»è¿‡ï¼Œæ˜¾ç¤ºé‡ç½®é€‰é¡¹
                    if st.session_state.email_button_clicked:
                        if st.button("ğŸ”„ é‡ç½®", key="reset_email_button", help="é‡ç½®é‚®ä»¶ç”ŸæˆçŠ¶æ€"):
                            st.session_state.email_button_clicked = False
                            if 'email_draft_content' in st.session_state:
                                del st.session_state.email_draft_content
                            st.rerun()

                # æ˜¾ç¤ºé‚®ä»¶å†…å®¹é¢„è§ˆ
                if 'email_draft_content' in st.session_state:
                    st.markdown("### é‚®ä»¶è‰ç¨¿é¢„è§ˆ")
                    st.text_area(
                        "é‚®ä»¶å†…å®¹",
                        st.session_state.email_draft_content,
                        height=300,
                        help="æ‚¨å¯ä»¥å¤åˆ¶æ­¤å†…å®¹åˆ°é‚®ä»¶å®¢æˆ·ç«¯",
                        key="diff_email_preview"
                    )
            else:
                st.success("æ²¡æœ‰å‘ç°å·®å¼‚")
        except Exception as e:
            st.error(f"æ— æ³•åŠ è½½å·®å¼‚æŠ¥å‘Š: {str(e)}")
    else:
        st.info("è¯·å…ˆå¤„ç†æ•°æ®")

# Logs Tab
with tab5:
    st.markdown("<h2 class='sub-header'>å¤„ç†æ—¥å¿—</h2>", unsafe_allow_html=True)

    # List all log files
    log_files = sorted([f for f in os.listdir(log_dir) if f.endswith('.log')], reverse=True)

    if log_files:
        selected_log = st.selectbox("é€‰æ‹©æ—¥å¿—æ–‡ä»¶", log_files)
        log_path = os.path.join(log_dir, selected_log)

        try:
            with open(log_path, 'r', encoding='utf-8') as f:
                log_content = f.read()

            # Add filter options
            filter_options = st.multiselect(
                "ç­›é€‰æ—¥å¿—çº§åˆ«",
                ["INFO", "WARNING", "ERROR"],
                default=["WARNING", "ERROR"]
            )

            # Filter log content
            filtered_lines = []
            for line in log_content.split('\n'):
                if any(level in line for level in filter_options):
                    filtered_lines.append(line)

            if filtered_lines:
                st.text_area("æ—¥å¿—å†…å®¹", '\n'.join(filtered_lines), height=400)
            else:
                st.info("æ²¡æœ‰ç¬¦åˆç­›é€‰æ¡ä»¶çš„æ—¥å¿—")

            # Download button for log file
            with open(log_path, "rb") as file:
                st.download_button(
                    label="ä¸‹è½½å®Œæ•´æ—¥å¿—æ–‡ä»¶",
                    data=file,
                    file_name=selected_log,
                    mime="text/plain"
                )
        except Exception as e:
            st.error(f"æ— æ³•è¯»å–æ—¥å¿—æ–‡ä»¶: {str(e)}")
    else:
        st.info("æ²¡æœ‰æ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")

# Footer
st.markdown("---")

# è‡ªåŠ¨ä¸‹è½½æ¯”å¯¹æŠ¥å‘Š
if 'show_download_button' in st.session_state and st.session_state.show_download_button:
    if 'auto_download_report' in st.session_state:
        st.success("å¤„ç†å®Œæˆï¼æ¯”å¯¹æŠ¥å‘Šå·²å‡†å¤‡å¥½ä¸‹è½½")

        col1, col2 = st.columns(2)

        with col1:
            # ä½¿ç”¨session stateæ¥ç®¡ç†ä¸‹è½½çŠ¶æ€ï¼Œé¿å…é¡µé¢è·³è½¬
            # åˆå§‹åŒ–ä¸‹è½½çŠ¶æ€
            if 'download_clicked' not in st.session_state:
                st.session_state.download_clicked = False

            # åˆ›å»ºä¸‹è½½æŒ‰é’®
            download_button = st.download_button(
                label="ğŸ“¥ ç‚¹å‡»ä¸‹è½½æ¯”å¯¹æŠ¥å‘Š",
                data=st.session_state.auto_download_report,
                file_name="æ¯”å¯¹æŠ¥å‘Š.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                key="auto_download_report_button",
                use_container_width=True,
                help="ç‚¹å‡»ä¸‹è½½å·®å¼‚æ¯”å¯¹æŠ¥å‘Šæ–‡ä»¶"
            )

            # å¦‚æœä¸‹è½½æŒ‰é’®è¢«ç‚¹å‡»ï¼Œæ˜¾ç¤ºåé¦ˆ
            if download_button:
                st.session_state.download_clicked = True
                success_container = st.container()
                with success_container:
                    st.success("âœ… æŠ¥å‘Šä¸‹è½½å·²å¼€å§‹")
                    st.info(" æ–‡ä»¶å°†ä¿å­˜åˆ°æ‚¨çš„é»˜è®¤ä¸‹è½½æ–‡ä»¶å¤¹")

        with col2:
            # ä½¿ç”¨session stateæ¥ç®¡ç†é‚®ä»¶ç”ŸæˆçŠ¶æ€ï¼Œé¿å…é¡µé¢è·³è½¬
            if 'show_email_button' in st.session_state and st.session_state.show_email_button:
                # æ£€æŸ¥æ˜¯å¦å·²ç»ç”Ÿæˆè¿‡é‚®ä»¶è‰ç¨¿
                if 'email_generated' not in st.session_state:
                    st.session_state.email_generated = False

                # ä½¿ç”¨ä¸åŒçš„æŒ‰é’®æ–‡æœ¬æ¥åæ˜ çŠ¶æ€
                button_label = "ğŸ”„ é‡æ–°ç”Ÿæˆé‚®ä»¶è‰ç¨¿" if st.session_state.email_generated else "ğŸ“§ ç”Ÿæˆé€šçŸ¥é‚®ä»¶è‰ç¨¿"

                if st.button(button_label, type="secondary", key="auto_generate_email_button", use_container_width=True, help="ç”Ÿæˆå¹¶æ‰“å¼€é‚®ä»¶å®¢æˆ·ç«¯"):
                    if 'email_draft_content' in st.session_state:
                        # æ ‡è®°é‚®ä»¶å·²ç”Ÿæˆ
                        st.session_state.email_generated = True

                        # ä½¿ç”¨å®¹å™¨æ¥æ˜¾ç¤ºçŠ¶æ€æ¶ˆæ¯ï¼Œé¿å…é¡µé¢è·³è½¬
                        success_container = st.container()
                        with success_container:
                            st.success("âœ… é‚®ä»¶è‰ç¨¿å·²å‡†å¤‡å°±ç»ª")

                        # å°è¯•æ‰“å¼€é‚®ä»¶å®¢æˆ·ç«¯
                        if open_email_client(st.session_state.email_draft_content):
                            info_container = st.container()
                            with info_container:
                                st.info("ğŸ“§ å·²æ‰“å¼€é»˜è®¤é‚®ä»¶å®¢æˆ·ç«¯")
                        else:
                            warning_container = st.container()
                            with warning_container:
                                st.warning("âš ï¸ æ— æ³•æ‰“å¼€é‚®ä»¶å®¢æˆ·ç«¯ï¼Œè¯·æŸ¥çœ‹ä¸‹æ–¹é‚®ä»¶å†…å®¹")
                    else:
                        error_container = st.container()
                        with error_container:
                            st.error("âŒ é‚®ä»¶è‰ç¨¿å†…å®¹ä¸å¯ç”¨ï¼Œè¯·é‡æ–°å¤„ç†æ•°æ®")

# æ˜¾ç¤ºè‡ªåŠ¨ç”Ÿæˆçš„é‚®ä»¶è‰ç¨¿é¢„è§ˆ - ä½¿ç”¨session stateæ§åˆ¶æ˜¾ç¤º
if 'email_draft_content' in st.session_state and st.session_state.email_draft_content:
    st.markdown("### ğŸ“§ é‚®ä»¶è‰ç¨¿å·²è‡ªåŠ¨ç”Ÿæˆ")
    with st.expander("æŸ¥çœ‹é‚®ä»¶å†…å®¹", expanded=False):
        st.text_area(
            "é‚®ä»¶å†…å®¹",
            st.session_state.email_draft_content,
            height=200,
            help="æ‚¨å¯ä»¥å¤åˆ¶æ­¤å†…å®¹åˆ°é‚®ä»¶å®¢æˆ·ç«¯",
            key="auto_email_preview",
            disabled=True  # è®¾ç½®ä¸ºåªè¯»ï¼Œé¿å…æ„å¤–ä¿®æ”¹
        )

st.markdown("Â© 2025 Checklistæ ¸å¯¹ç³»ç»Ÿ | ç‰ˆæœ¬ 1.2")